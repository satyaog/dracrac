[{"paper_id": "8930a484d8aad1cf022ec0f1a62713d5", "title": "Maximum State Entropy Exploration using Predecessor and Successor Representations", "abstract": "Animals have a developed ability to explore that aids them in important tasks such as locating food, exploring for shelter, and finding misplaced items. These exploration skills necessarily track where they have been so that they can plan for finding items with relative efficiency. Contemporary exploration algorithms often learn a less efficient exploration strategy because they either condition only on the current state or simply rely on making random open-loop exploratory moves. In this work, we propose $\\eta\\psi$-Learning, a method to learn efficient exploratory policies by conditioning on past episodic experience to make the next exploratory move. Specifically, $\\eta\\psi$-Learning learns an exploration policy that maximizes the entropy of the state visitation distribution of a single trajectory. Furthermore, we demonstrate how variants of the predecessor representation and successor representations can be combined to predict the state visitation entropy. Our experiments demonstrate the efficacy of $\\eta\\psi$-Learning to strategically explore the environment and maximize the state coverage with limited samples.", "authors": [{"author": {"author_id": "a873db153082ee433ec02f71f0aaa702", "name": "Arnav Kumar Jain", "links": [{"type": "openreview", "link": "~Arnav_Kumar_Jain2"}, {"type": "semantic_scholar", "link": "7284555"}]}, "affiliations": [{"institution_id": "099386ee291a46438cd2f3c2e05267c8", "name": "University de Montreal", "category": "academia"}]}, {"author": {"author_id": "ce65563792a1927b992b7801c86797e8", "name": "Lucas Lehnert", "links": [{"type": "openreview", "link": "~Lucas_Lehnert1"}, {"type": "semantic_scholar", "link": "2284988047"}, {"type": "semantic_scholar", "link": "39251318"}]}, "affiliations": [{"institution_id": "53ebc9c43621e396de19895e697cb413", "name": "Meta", "category": "unknown"}]}, {"author": {"author_id": "d7996e27da7f5c147d15bf0ee70b7c0c", "name": "Irina Rish", "links": [{"type": "!semantic_scholar", "link": "2064747582"}, {"type": "bio", "link": "irina-rish"}, {"type": "email.mila", "link": "irina.rish@mila.quebec"}, {"type": "mag", "link": "1653753694"}, {"type": "mag", "link": "2997858046"}, {"type": "mag", "link": "3025417811"}, {"type": "openreview", "link": "~Irina_Rish1"}, {"type": "semantic_scholar", "link": "2109771"}, {"type": "semantic_scholar", "link": "2239232896"}, {"type": "semantic_scholar", "link": "2284772297"}, {"type": "wpid_en", "link": "37584"}, {"type": "wpid_fr", "link": "37580"}, {"type": "xplore", "link": "37268997000"}]}, "affiliations": [{"institution_id": "099386ee291a46438cd2f3c2e05267c8", "name": "University de Montreal", "category": "academia"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "099386ee291a46438cd2f3c2e05267c8", "name": "University de Montreal", "category": "academia"}]}], "releases": [{"venue": {"venue_id": "729777f3ada3f43e66622a8ed8f2bf62", "name": "Advances in Neural Information Processing Systems 36  (NeurIPS 2023)", "type": "unknown", "date": {"text": "2023", "timestamp": 1672549200, "precision": 1}, "links": [], "publisher": "Curran Associates, Inc.", "series": "", "volume": "36"}, "peer_reviewed": true, "status": "published", "pages": "49991\u201350019"}], "topics": [{"name": "Successor Representation"}, {"name": "Computer Science"}, {"name": "State Visitation Distribution"}, {"name": "Maximum state entropy exploration"}, {"name": "Non-Markovian exploration"}, {"name": "Reinforcement Learning"}], "links": [{"type": "html.official", "link": "https://proceedings.neurips.cc//paper_files/paper/2023/hash/9c7900fac04a701cbed83256b76dbaa3-Abstract-Conference.html"}, {"type": "pdf.official", "link": "https://proceedings.neurips.cc/paper_files/paper/2023/file/9c7900fac04a701cbed83256b76dbaa3-Paper-Conference.pdf"}, {"type": "doi.abstract", "link": "10.48550/arXiv.2306.14808", "url": "https://doi.org/10.48550/arXiv.2306.14808"}, {"type": "openreview.abstract", "link": "inE5hW4tQ0", "url": "https://openreview.net/forum?id=inE5hW4tQ0"}, {"type": "openreview.abstract", "link": "tFsxtqGmkn", "url": "https://openreview.net/forum?id=tFsxtqGmkn"}, {"type": "openreview.pdf", "link": "inE5hW4tQ0", "url": "https://openreview.net/pdf?id=inE5hW4tQ0"}, {"type": "openreview.pdf", "link": "tFsxtqGmkn", "url": "https://openreview.net/pdf?id=tFsxtqGmkn"}, {"type": "arxiv.abstract", "link": "2306.14808", "url": "https://arxiv.org/abs/2306.14808"}, {"type": "arxiv.pdf", "link": "2306.14808", "url": "https://arxiv.org/pdf/2306.14808.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2306-14808", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2306-14808"}, {"type": "pdf", "link": "http://export.arxiv.org/pdf/2306.14808"}, {"type": "semantic_scholar.abstract", "link": "6b8e98792e4af57687939156c07b99cd12187f89", "url": "https://www.semanticscholar.org/paper/6b8e98792e4af57687939156c07b99cd12187f89"}, {"type": "corpusid", "link": "259261970"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "aee5cf2b2406f3cb6cb6e24f08d60b00", "title": "Towards Learning to Imitate from a Single Video Demonstration", "abstract": "Agents that can learn to imitate given video observation -- \\emph{without direct access to state or action information} are more applicable to learning in the natural world. However, formulating a reinforcement learning (RL) agent that facilitates this goal remains a significant challenge. We approach this challenge using contrastive training to learn a reward function comparing an agent's behaviour with a single demonstration. We use a Siamese recurrent neural network architecture to learn rewards in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we also find that the inclusion of multi-task data and additional image encoding losses improve the temporal consistency of the learned rewards and, as a result, significantly improves policy learning. We demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D and a quadruped and a humanoid in 3D. We show that our method outperforms current state-of-the-art techniques in these environments and can learn to imitate from a single video demonstration.", "authors": [{"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": []}, {"author": {"author_id": "b223f9e59d03cccdef6c12fd6c51858c", "name": "Florian Golemo", "links": [{"type": "openreview", "link": "~Florian_Golemo1"}, {"type": "semantic_scholar", "link": "2066832277"}, {"type": "semantic_scholar", "link": "2970150"}, {"type": "xplore", "link": "37086270539"}]}, "affiliations": []}, {"author": {"author_id": "d4294b54e10232088f5feff243c4e293", "name": "Christopher Pal", "links": [{"type": "bio", "link": "pal-christopher"}, {"type": "email.mila", "link": "christopher.pal@mila.quebec"}, {"type": "mag", "link": "2168619732"}, {"type": "mag", "link": "2900737546"}, {"type": "mag", "link": "2910816495"}, {"type": "mag", "link": "2923024885"}, {"type": "openreview", "link": "~Christopher_Pal1"}, {"type": "semantic_scholar", "link": "1972076"}, {"type": "semantic_scholar", "link": "2061666783"}, {"type": "semantic_scholar", "link": "2061666816"}, {"type": "semantic_scholar", "link": "2061666830"}, {"type": "semantic_scholar", "link": "2132432404"}, {"type": "semantic_scholar", "link": "2242198325"}, {"type": "semantic_scholar", "link": "2243333924"}, {"type": "semantic_scholar", "link": "2249529376"}, {"type": "semantic_scholar", "link": "2252966270"}, {"type": "semantic_scholar", "link": "2265966393"}, {"type": "semantic_scholar", "link": "2275240361"}, {"type": "semantic_scholar", "link": "2282539195"}, {"type": "semantic_scholar", "link": "2283771332"}, {"type": "semantic_scholar", "link": "2288590587"}, {"type": "semantic_scholar", "link": "2294173613"}, {"type": "semantic_scholar", "link": "2294691864"}, {"type": "semantic_scholar", "link": "2295623112"}, {"type": "semantic_scholar", "link": "2310239092"}, {"type": "semantic_scholar", "link": "98109738"}, {"type": "wpid_en", "link": "362"}, {"type": "wpid_fr", "link": "329"}, {"type": "xplore", "link": "37086726955"}, {"type": "xplore", "link": "37282879700"}]}, "affiliations": []}], "releases": [{"venue": {"venue_id": "7dbbda687595af5ebd3fa932628bc5a1", "name": "J. Mach. Learn. Res.", "type": "journal", "date": {"text": "2023", "timestamp": 1672549200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": "78:1-78:26"}, {"venue": {"venue_id": "390e40c89f9824060add53bea5507663", "name": "Journal of Machine Learning Research", "type": "unknown", "date": {"text": "2023", "timestamp": 1672549200, "precision": 1}, "links": [], "publisher": "JMLR", "series": "", "volume": "24"}, "peer_reviewed": true, "status": "published", "pages": "1\u201326"}], "topics": [{"name": "Computer Science"}, {"name": "Mathematics"}], "links": [{"type": "html.official", "link": "https://jmlr.org/papers/v24/21-1174.html"}, {"type": "pdf.official", "link": "https://jmlr.org/papers/volume24/21-1174/21-1174.pdf"}, {"type": "arxiv.abstract", "link": "1901.07186", "url": "https://arxiv.org/abs/1901.07186"}, {"type": "arxiv.pdf", "link": "1901.07186", "url": "https://arxiv.org/pdf/1901.07186.pdf"}, {"type": "dblp.abstract", "link": "journals/jmlr/BersethGP23", "url": "https://dblp.uni-trier.de/rec/journals/jmlr/BersethGP23"}, {"type": "html", "link": "http://jmlr.org/papers/v24/21-1174.html"}, {"type": "semantic_scholar.abstract", "link": "f7a73ca6b0d0491a49516676f991627a548fa1e4", "url": "https://www.semanticscholar.org/paper/f7a73ca6b0d0491a49516676f991627a548fa1e4"}, {"type": "corpusid", "link": "239338757"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "88054901fa80690fd920340ffbbfd31f", "title": "Hierarchical Reinforcement Learning for Precise Soccer Shooting Skills using a Quadrupedal Robot", "abstract": "We address the problem of enabling quadrupedal robots to perform precise shooting skills in the real world using reinforcement learning. Developing algorithms to enable a legged robot to shoot a soccer ball to a given target is a challenging problem that combines robot motion control and planning into one task. To solve this problem, we need to consider the dynamics limitation and motion stability during the control of a dynamic legged robot. Moreover, we need to consider motion planning to shoot the hard-to-model deformable ball rolling on the ground with uncertain friction to a desired location. In this paper, we propose a hierarchical framework that leverages deep reinforcement learning to train (a) a robust motion control policy that can track arbitrary motions and (b) a planning policy to decide the desired kicking motion to shoot a soccer ball to a target. We deploy the proposed framework on an A1 quadrupedal robot and enable it to accurately shoot the ball to random targets in the real world.", "authors": [{"author": {"author_id": "9245ee71cc36c8013b0d725e790995ae", "name": "Yandong Ji", "links": [{"type": "orcid", "link": "0000-0002-6948-7465"}, {"type": "semantic_scholar", "link": "2114169750"}, {"type": "xplore", "link": "37087473472"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "93cc03207aee2f4d30a097baf4d0cc7e", "name": "Zhongyu Li", "links": [{"type": "semantic_scholar", "link": "1491078398"}, {"type": "xplore", "link": "37088691308"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "b4e15fef26594cc7d57c7728b0857270", "name": "Yinan Sun", "links": [{"type": "semantic_scholar", "link": "2108940190"}, {"type": "xplore", "link": "37089661008"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "c29bd8289e233e797e229436c937a3eb", "name": "Xue Bin Peng", "links": [{"type": "semantic_scholar", "link": "32200465"}, {"type": "xplore", "link": "37086454470"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "db8d2cd3248301ddbe53e438c1f820a5", "name": "Sergey Levine", "links": [{"type": "openreview", "link": "~Sergey_Levine1"}, {"type": "semantic_scholar", "link": "1736651"}, {"type": "semantic_scholar", "link": "2249615151"}, {"type": "semantic_scholar", "link": "2257194331"}, {"type": "semantic_scholar", "link": "2279022150"}, {"type": "xplore", "link": "37085481973"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "0b1847fd61d72cd82f9a3130f23c9c94", "name": "Universit\u00e9 de Montr\u00e9al, Mila", "category": "unknown"}, {"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}]}, {"author": {"author_id": "823991839546214e31e46c9c086203cb", "name": "Koushil Sreenath", "links": [{"type": "orcid", "link": "0000-0002-5346-3637"}, {"type": "semantic_scholar", "link": "144116765"}, {"type": "xplore", "link": "37563179200"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "2238a30b81de098bf9065974d140d407", "name": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "type": "journal", "date": {"text": "2022-10-23", "timestamp": 1666497600, "precision": 3}, "links": [], "publisher": "IEEE", "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": "1479-1486"}], "topics": [{"name": "Policy Planning"}, {"name": "Motor Control"}, {"name": "Path Planning"}, {"name": "Ball Position"}, {"name": "Dynamics"}, {"name": "Robot motion"}, {"name": "Time Step"}, {"name": "Bezier Curve"}, {"name": "Computer Science"}, {"name": "Random Force"}, {"name": "Legged locomotion"}, {"name": "Simulation Training"}, {"name": "Time Span"}, {"name": "Low-pass"}, {"name": "Soccer Shooting"}, {"name": "Stance Leg"}, {"name": "Target tracking"}, {"name": "Quadrupedal robots"}, {"name": "Robot Motion"}, {"name": "Robust Policy"}, {"name": "Hierarchical Framework"}, {"name": "Humanoid Robot"}, {"name": "Reinforcement learning"}, {"name": "Model-based Methods"}, {"name": "Past Conditions"}, {"name": "Legged Robots"}, {"name": "Deep Reinforcement Learning"}, {"name": "Robotic Arm"}, {"name": "Motion In Order"}, {"name": "Real Robot"}, {"name": "Model-free Reinforcement Learning"}, {"name": "Stable Motion"}, {"name": "Soccer Ball"}, {"name": "End-effector Position"}, {"name": "Engineering"}, {"name": "Planning"}, {"name": "Quadruped Robot"}, {"name": "Robot State"}, {"name": "Hierarchical Reinforcement Learning"}], "links": [{"type": "doi.abstract", "link": "10.1109/IROS47612.2022.9981984", "url": "https://doi.org/10.1109/IROS47612.2022.9981984"}, {"type": "arxiv.abstract", "link": "2208.01160", "url": "https://arxiv.org/abs/2208.01160"}, {"type": "arxiv.pdf", "link": "2208.01160", "url": "https://arxiv.org/pdf/2208.01160.pdf"}, {"type": "dblp.abstract", "link": "conf/iros/JiLSPLBS22", "url": "https://dblp.uni-trier.de/rec/conf/iros/JiLSPLBS22"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2208-01160", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2208-01160"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2208.01160"}, {"type": "semantic_scholar.abstract", "link": "844c69fd25f016a7ba9b64255c65e15c7d1f6fc4", "url": "https://www.semanticscholar.org/paper/844c69fd25f016a7ba9b64255c65e15c7d1f6fc4"}, {"type": "corpusid", "link": "251253136"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "c1241147dab9dd2fd978aa49cdbd82b3", "title": "ASHA: Assistive Teleoperation via Human-in-the-Loop Reinforcement Learning", "abstract": "Building assistive interfaces for controlling robots through arbitrary, high-dimensional, noisy inputs (e.g., webcam images of eye gaze) can be challenging, especially when it involves inferring the user's desired action in the absence of a natural \u2018default\u2019 interface. Reinforcement learning from online user feedback on the system's performance presents a natural solution to this problem, and enables the interface to adapt to individual users. However, this approach tends to require a large amount of human-in-the-loop training data, especially when feedback is sparse. We propose a hierarchical solution that learns efficiently from sparse user feedback: we use offline pre-training to acquire a latent embedding space of useful, high-level robot behaviors, which, in turn, enables the system to focus on using online user feedback to learn a mapping from user inputs to desired high-level behaviors. The key insight is that access to a pre-trained policy enables the system to learn more from sparse rewards than a na\u00efve RL algorithm: using the pre-trained policy, the system can make use of successful task executions to relabel, in hindsight, what the user actually meant to do during unsuccessful executions. We evaluate our method primarily through a user study with 12 participants who perform tasks in three simulated robotic manipulation domains using a webcam and their eye gaze: flipping light switches, opening a shelf door to reach objects inside, and rotating a valve. The results show that our method successfully learns to map 128-dimensional gaze features to 7-dimensional joint torques from sparse rewards in under 10 minutes of online training, and seamlessly helps users who employ different gaze strategies, while adapting to distributional shift in webcam inputs, tasks, and environments", "authors": [{"author": {"author_id": "b1ea6f2cc24ebbb820f4a8eade6a8384", "name": "Sean Chen", "links": [{"type": "semantic_scholar", "link": "2141325983"}, {"type": "xplore", "link": "37089447557"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}]}, {"author": {"author_id": "dec1fab1399c0f1b5759bde7f308875e", "name": "Jensen Gao", "links": [{"type": "semantic_scholar", "link": "2110482632"}, {"type": "semantic_scholar", "link": "2238154243"}, {"type": "xplore", "link": "37089448284"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}]}, {"author": {"author_id": "828a063b62edad8579b1014f5d81cc78", "name": "Siddharth Reddy", "links": [{"type": "semantic_scholar", "link": "37372079"}, {"type": "xplore", "link": "37088506876"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "e06e180f6ddb16b17a1f8925331b98fb", "name": "Anca Dragan", "links": [{"type": "openreview", "link": "~Anca_Dragan1"}, {"type": "semantic_scholar", "link": "2064066935"}, {"type": "semantic_scholar", "link": "2745001"}, {"type": "xplore", "link": "37960625200"}]}, "affiliations": [{"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "db8d2cd3248301ddbe53e438c1f820a5", "name": "Sergey Levine", "links": [{"type": "openreview", "link": "~Sergey_Levine1"}, {"type": "semantic_scholar", "link": "1736651"}, {"type": "semantic_scholar", "link": "2249615151"}, {"type": "semantic_scholar", "link": "2257194331"}, {"type": "semantic_scholar", "link": "2279022150"}, {"type": "xplore", "link": "37085481973"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "11413c609c9b35b5dc3f92e287543e6c", "name": "2022 International Conference on Robotics and Automation (ICRA)", "type": "journal", "date": {"text": "2022-05-23", "timestamp": 1653278400, "precision": 3}, "links": [], "publisher": "IEEE", "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": "7505-7512"}], "topics": [{"name": "User Control"}, {"name": "User Feedback"}, {"name": "End Of Episode"}, {"name": "Optimal Policy"}, {"name": "Computer Science"}, {"name": "POMDP"}, {"name": "Eye Gaze"}, {"name": "Uniform Distribution"}, {"name": "System Performance"}, {"name": "Noisy Input"}, {"name": "Latent Embedding"}, {"name": "User Input"}, {"name": "Webcams"}, {"name": "Robot Manipulator"}, {"name": "Simulation Domain"}, {"name": "Embedding Space"}, {"name": "Distribution Of Tasks"}, {"name": "Equation Modeling"}, {"name": "Latent Variable Model"}, {"name": "Reinforcement Learning Algorithm"}, {"name": "Task Success"}, {"name": "End Of Each Episode"}, {"name": "Partially Observable Markov Decision Process"}, {"name": "User Study"}, {"name": "Training"}, {"name": "System performance"}, {"name": "Task Shifting"}, {"name": "Latent Space"}, {"name": "Individual Users"}, {"name": "Reinforcement learning"}, {"name": "Eye Contact"}, {"name": "Human in the loop"}, {"name": "Hindsight"}, {"name": "Valves"}, {"name": "Pre-training Tasks"}, {"name": "Successional Trajectories"}, {"name": "Training data"}, {"name": "Domain Switching"}, {"name": "Minutes Of Training"}, {"name": "Online Users"}, {"name": "Input Encoding"}], "links": [{"type": "doi.abstract", "link": "10.1109/icra46639.2022.9812442", "url": "https://doi.org/10.1109/icra46639.2022.9812442"}, {"type": "arxiv.abstract", "link": "2202.02465", "url": "https://arxiv.org/abs/2202.02465"}, {"type": "arxiv.pdf", "link": "2202.02465", "url": "https://arxiv.org/pdf/2202.02465.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2202-02465", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2202-02465"}, {"type": "pdf", "link": "http://export.arxiv.org/pdf/2202.02465"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2202.02465"}, {"type": "semantic_scholar.abstract", "link": "a1189ba5d86d32bc5fecd32ee905f8ff4767cbdb", "url": "https://www.semanticscholar.org/paper/a1189ba5d86d32bc5fecd32ee905f8ff4767cbdb"}, {"type": "corpusid", "link": "237263044"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}]
