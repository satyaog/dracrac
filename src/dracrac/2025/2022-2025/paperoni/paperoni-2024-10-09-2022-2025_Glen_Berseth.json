[{"paper_id": "a4dcc843c634b539da4c7c2644791032", "title": "Intelligent Switching for Reset-Free RL", "abstract": "In the real world, the strong episode resetting mechanisms that are needed to train agents in simulation are unavailable. The \\textit{resetting} assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional handcrafted mechanisms or human interventions. Recent work aims to train agents (\\textit{forward}) with learned resets by constructing a second (\\textit{backward}) agent that returns the forward agent to the initial state. We find that the termination and timing of the transitions between these two agents are crucial for algorithm success. With this in mind, we create a new algorithm, Reset Free RL with Intelligently Switching Controller (RISC) which intelligently switches between the two agents based on the agent's confidence in achieving its current goal. Our new method achieves state-of-the-art performance on several challenging environments for reset-free RL.", "authors": [{"author": {"author_id": "dc206682d3ddebdc07d73cdb8593b24f", "name": "Darshan Patil", "links": [{"type": "openreview", "link": "~Darshan_Patil1"}, {"type": "semantic_scholar", "link": "2111849117"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "9615ba8643982f3eb958389ddfc88e2b", "name": "Janarthanan Rajendran", "links": [{"type": "openreview", "link": "~Janarthanan_Rajendran1"}, {"type": "openreview", "link": "~Janarthanan_Rajendran2"}, {"type": "semantic_scholar", "link": "10197529"}]}, "affiliations": [{"institution_id": "48314fb4f8bc3694a5542910d319f0b8", "name": "Dalhousie University", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "0a4595cdfa4e6d9d22cc74f6a6a2666f", "name": "Canada CIFAR AI Chair", "category": "unknown"}, {"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "95533338b3953432f8ec4743f02341b3", "name": "Sarath Chandar", "links": [{"type": "!semantic_scholar", "link": "1783528"}, {"type": "!semantic_scholar", "link": "92768738"}, {"type": "bio", "link": "sarath-chandar"}, {"type": "email.mila", "link": "sarath.chandar@mila.quebec"}, {"type": "mag", "link": "2044221253"}, {"type": "mag", "link": "2616279507"}, {"type": "mag", "link": "2994242150"}, {"type": "mag", "link": "3035505426"}, {"type": "openreview", "link": "~Sarath_Chandar1"}, {"type": "semantic_scholar", "link": "123607932"}, {"type": "semantic_scholar", "link": "144631588"}, {"type": "wpid_en", "link": "1777"}, {"type": "wpid_fr", "link": "1780"}, {"type": "xplore", "link": "37085932450"}, {"type": "xplore", "link": "37086254634"}, {"type": "xplore", "link": "37087078794"}]}, "affiliations": [{"institution_id": "0a4595cdfa4e6d9d22cc74f6a6a2666f", "name": "Canada CIFAR AI Chair", "category": "unknown"}, {"institution_id": "316e18a8da455bed7b7e9221fd7b6ae9", "name": "\u00c9cole Polytechnique de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}, {"venue": {"venue_id": "213ea6e666cc9ccdc524f4a3ce35e592", "name": "International Conference on Learning Representations", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": "abs/2405.01684"}, "peer_reviewed": true, "status": "published", "pages": null}, {"venue": {"venue_id": "6b00fec8c0d7598bda0e96167eaabbc1", "name": "ICLR", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Computer Science"}, {"name": "Reset-Free RL"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2405.01684", "url": "https://doi.org/10.48550/arXiv.2405.01684"}, {"type": "openreview.abstract", "link": "Nq45xeghcL", "url": "https://openreview.net/forum?id=Nq45xeghcL"}, {"type": "openreview.pdf", "link": "Nq45xeghcL", "url": "https://openreview.net/pdf?id=Nq45xeghcL"}, {"type": "arxiv.abstract", "link": "2405.01684", "url": "https://arxiv.org/abs/2405.01684"}, {"type": "arxiv.pdf", "link": "2405.01684", "url": "https://arxiv.org/pdf/2405.01684.pdf"}, {"type": "dblp.abstract", "link": "conf/iclr/PatilRBC24", "url": "https://dblp.uni-trier.de/rec/conf/iclr/PatilRBC24"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2405-01684", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2405-01684"}, {"type": "html", "link": "https://openreview.net/forum?id=Nq45xeghcL"}, {"type": "semantic_scholar.abstract", "link": "1463eb69e412ee1e44bfcb75e666cfc1dc0ba3fc", "url": "https://www.semanticscholar.org/paper/1463eb69e412ee1e44bfcb75e666cfc1dc0ba3fc"}, {"type": "semantic_scholar.abstract", "link": "906163e86c50998e2ab2223ed2f897f49175cc2d", "url": "https://www.semanticscholar.org/paper/906163e86c50998e2ab2223ed2f897f49175cc2d"}, {"type": "corpusid", "link": "269587774"}, {"type": "corpusid", "link": "270370048"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "b9c8f2cad469f240448d67ad7c0db3d2", "title": "Closing the Gap between TD Learning and Supervised Learning - A Generalisation Point of View", "abstract": "Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching and generalisation reveals why we should not expect SL-based RL methods to perform stitching, even in the limit of large datasets and models. Based on this analysis, we construct new datasets to explicitly test for this property, revealing that SL-based methods lack this stitching property and hence fail to perform combinatorial generalization. Nonetheless, the connection between stitching and combinatorial generalisation also suggests a simple remedy for improving generalisation in SL: data augmentation. We propose a temporal data augmentation and demonstrate that adding it to SL-based methods enables them to successfully complete tasks not seen together during training. On a high level, this connection illustrates the importance of combinatorial generalization for data efficiency in time-series data beyond tasks beyond RL, like audio, video, or text.", "authors": [{"author": {"author_id": "e23ef866bc9388321417ac2b3e4d19d2", "name": "Raj Ghugare", "links": [{"type": "openreview", "link": "~Raj_Ghugare1"}, {"type": "semantic_scholar", "link": "2185406398"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "a8fbe3b4d9281d30e20479256400ebd8", "name": "Matthieu Geist", "links": [{"type": "openreview", "link": "~Matthieu_Geist1"}, {"type": "semantic_scholar", "link": "1737555"}, {"type": "semantic_scholar", "link": "2253609155"}, {"type": "semantic_scholar", "link": "2269464568"}]}, "affiliations": []}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "92dd61a3c7714571a096058c04c40646", "name": "Benjamin Eysenbach", "links": [{"type": "openreview", "link": "~Benjamin_Eysenbach1"}, {"type": "semantic_scholar", "link": "8140754"}]}, "affiliations": [{"institution_id": "7ec86b4c6bba99408f0ee04995bfa897", "name": "Princeton University", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}, {"venue": {"venue_id": "6b00fec8c0d7598bda0e96167eaabbc1", "name": "ICLR", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Computer Science"}, {"name": "data augmentation"}, {"name": "reinforcement learning"}, {"name": "stitching"}, {"name": "reinforcement learning decision transformers"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2401.11237", "url": "https://doi.org/10.48550/arXiv.2401.11237"}, {"type": "openreview.abstract", "link": "1SJZVCahQW", "url": "https://openreview.net/forum?id=1SJZVCahQW"}, {"type": "openreview.abstract", "link": "S1aOVW3EWv", "url": "https://openreview.net/forum?id=S1aOVW3EWv"}, {"type": "openreview.abstract", "link": "qg5JENs0N4", "url": "https://openreview.net/forum?id=qg5JENs0N4"}, {"type": "openreview.pdf", "link": "1SJZVCahQW", "url": "https://openreview.net/pdf?id=1SJZVCahQW"}, {"type": "openreview.pdf", "link": "S1aOVW3EWv", "url": "https://openreview.net/pdf?id=S1aOVW3EWv"}, {"type": "openreview.pdf", "link": "qg5JENs0N4", "url": "https://openreview.net/pdf?id=qg5JENs0N4"}, {"type": "arxiv.abstract", "link": "2401.11237", "url": "https://arxiv.org/abs/2401.11237"}, {"type": "arxiv.pdf", "link": "2401.11237", "url": "https://arxiv.org/pdf/2401.11237.pdf"}, {"type": "dblp.abstract", "link": "conf/iclr/GhugareGBE24", "url": "https://dblp.uni-trier.de/rec/conf/iclr/GhugareGBE24"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2401-11237", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2401-11237"}, {"type": "html", "link": "https://openreview.net/forum?id=qg5JENs0N4"}, {"type": "semantic_scholar.abstract", "link": "9cc6151bad5d027b00ff6931e2ab7012beffa841", "url": "https://www.semanticscholar.org/paper/9cc6151bad5d027b00ff6931e2ab7012beffa841"}, {"type": "corpusid", "link": "267068467"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "d3da82b02323ec64ced8fb80956eca32", "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers", "abstract": "", "authors": [{"author": {"author_id": "e23ef866bc9388321417ac2b3e4d19d2", "name": "Raj Ghugare", "links": [{"type": "openreview", "link": "~Raj_Ghugare1"}, {"type": "semantic_scholar", "link": "2185406398"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "2e7644e4488777eb735a190201699cd0", "name": "Mila - Quebec AI Institute", "category": "unknown"}]}, {"author": {"author_id": "9e2eebd82ff2df791c97661885142f8d", "name": "Santiago Miret", "links": [{"type": "openreview", "link": "~Santiago_Miret1"}, {"type": "semantic_scholar", "link": "2237988122"}, {"type": "semantic_scholar", "link": "2259929505"}, {"type": "semantic_scholar", "link": "2273773286"}, {"type": "semantic_scholar", "link": "51895312"}]}, "affiliations": [{"institution_id": "6ec952c1c04eb0612c33c2f19a4c99b7", "name": "Intel Labs", "category": "unknown"}]}, {"author": {"author_id": "d66c37481612a532aff2122c259f0740", "name": "Adriana Hugessen", "links": [{"type": "openreview", "link": "~Adriana_Hugessen1"}, {"type": "semantic_scholar", "link": "2253652590"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "2e7644e4488777eb735a190201699cd0", "name": "Mila - Quebec AI Institute", "category": "unknown"}]}, {"author": {"author_id": "87ac14bb4d4583abfc322f1fc3c0eaa4", "name": "Mariano Phielipp", "links": [{"type": "openreview", "link": "~Mariano_Phielipp2"}, {"type": "semantic_scholar", "link": "2253655183"}, {"type": "semantic_scholar", "link": "2482400"}]}, "affiliations": [{"institution_id": "6ec952c1c04eb0612c33c2f19a4c99b7", "name": "Intel Labs", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "2e7644e4488777eb735a190201699cd0", "name": "Mila - Quebec AI Institute", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}], "topics": [{"name": "Physics"}, {"name": "chemistry"}, {"name": "Computer Science"}, {"name": "molecular docking"}, {"name": "reinforcement learning"}, {"name": "language models"}, {"name": "pytdc"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2310.02902", "url": "https://doi.org/10.48550/arXiv.2310.02902"}, {"type": "openreview.abstract", "link": "O8mZO2ri33", "url": "https://openreview.net/forum?id=O8mZO2ri33"}, {"type": "openreview.abstract", "link": "nqlymMx42E", "url": "https://openreview.net/forum?id=nqlymMx42E"}, {"type": "openreview.pdf", "link": "O8mZO2ri33", "url": "https://openreview.net/pdf?id=O8mZO2ri33"}, {"type": "openreview.pdf", "link": "nqlymMx42E", "url": "https://openreview.net/pdf?id=nqlymMx42E"}, {"type": "arxiv.abstract", "link": "2310.02902", "url": "https://arxiv.org/abs/2310.02902"}, {"type": "arxiv.pdf", "link": "2310.02902", "url": "https://arxiv.org/pdf/2310.02902.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2310-02902", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2310-02902"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2310.02902"}, {"type": "semantic_scholar.abstract", "link": "8923aec569a13f94148e3e90a94c68730f6ad03d", "url": "https://www.semanticscholar.org/paper/8923aec569a13f94148e3e90a94c68730f6ad03d"}, {"type": "corpusid", "link": "263620293"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "85fe32ed5c3365ddbea06d54544182b1", "title": "Improving Intrinsic Exploration by Creating Stationary Objectives", "abstract": "", "authors": [{"author": {"author_id": "9e8e03d8b383bafb594acdf3b9b91241", "name": "Roger Creus Castanyer", "links": [{"type": "openreview", "link": "~Roger_Creus_Castanyer1"}, {"type": "semantic_scholar", "link": "2053524695"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "3b267db4aea551ea093661ef7519c1ec", "name": "Mila Qu\u00e9bec AI Institute", "category": "academia"}]}, {"author": {"author_id": "8a36f7d2afa669eb753077880232afb5", "name": "Joshua Romoff", "links": [{"type": "openreview", "link": "~Joshua_Romoff1"}, {"type": "semantic_scholar", "link": "8365320"}]}, "affiliations": []}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "3b267db4aea551ea093661ef7519c1ec", "name": "Mila Qu\u00e9bec AI Institute", "category": "academia"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}, {"venue": {"venue_id": "6b00fec8c0d7598bda0e96167eaabbc1", "name": "ICLR", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Stationarity"}, {"name": "Computer Science"}, {"name": "Exploration"}, {"name": "Reinforcement Learning"}, {"name": "Intrinsic Rewards"}, {"name": "Intrinsic Objectives"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2310.18144", "url": "https://doi.org/10.48550/arXiv.2310.18144"}, {"type": "openreview.abstract", "link": "V7Ao0FdXEn", "url": "https://openreview.net/forum?id=V7Ao0FdXEn"}, {"type": "openreview.abstract", "link": "YbZxT0SON4", "url": "https://openreview.net/forum?id=YbZxT0SON4"}, {"type": "openreview.pdf", "link": "V7Ao0FdXEn", "url": "https://openreview.net/pdf?id=V7Ao0FdXEn"}, {"type": "openreview.pdf", "link": "YbZxT0SON4", "url": "https://openreview.net/pdf?id=YbZxT0SON4"}, {"type": "arxiv.abstract", "link": "2310.18144", "url": "https://arxiv.org/abs/2310.18144"}, {"type": "arxiv.pdf", "link": "2310.18144", "url": "https://arxiv.org/pdf/2310.18144.pdf"}, {"type": "dblp.abstract", "link": "conf/iclr/CastanyerRB24", "url": "https://dblp.uni-trier.de/rec/conf/iclr/CastanyerRB24"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2310-18144", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2310-18144"}, {"type": "html", "link": "https://openreview.net/forum?id=YbZxT0SON4"}, {"type": "semantic_scholar.abstract", "link": "99f9198f9d7ae72d09c9ddc7a8451f119dfeab84", "url": "https://www.semanticscholar.org/paper/99f9198f9d7ae72d09c9ddc7a8451f119dfeab84"}, {"type": "corpusid", "link": "264555396"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "ad2e4e26a1db7eb96aadd65589ef313f", "title": "Torque-Based Deep Reinforcement Learning for Task-and-Robot Agnostic Learning on Bipedal Robots Using Sim-to-Real Transfer", "abstract": "In this letter, we review the question of which action space is best suited for controlling a real biped robot in combination with Sim2Real training. Position control has been popular as it has been shown to be more sample efficient and intuitive to combine with other planning algorithms. However, for position control, gain tuning is required to achieve the best possible policy performance. We show that, instead, using a torque-based action space enables task-and-robot agnostic learning with less parameter tuning and mitigates the sim-to-reality gap by taking advantage of torque control's inherent compliance. Also, we accelerate the torque-based-policy training process by pre-training the policy to remain upright by compensating for gravity. The letter showcases the first successful sim-to-real transfer of a torque-based deep reinforcement learning policy on a real human-sized biped robot.", "authors": [{"author": {"author_id": "bf1cd394cc29bbc5a0b5f98c777f785d", "name": "Donghyeon Kim", "links": [{"type": "semantic_scholar", "link": "1799288522"}, {"type": "xplore", "link": "37087324205"}]}, "affiliations": [{"institution_id": "47fe66dce198ee02f0f7eab4fed4b64c", "name": "Department of Intelligence and Information, Seoul National University, Seoul, Republic of Korea", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "45223f65c0fc88e938aa08c86ddc3973", "name": "Universit\u00e9 de Montr\u00e9al, Quebec, Canada", "category": "unknown"}, {"institution_id": "4cfe82f20f6016691ccda3e248335bd9", "name": "Canada", "category": "unknown"}]}, {"author": {"author_id": "999a07a3603e51fab1b05107bab999f7", "name": "Mathew Schwartz", "links": [{"type": "semantic_scholar", "link": "2980585"}, {"type": "xplore", "link": "37085580327"}]}, "affiliations": [{"institution_id": "3ca9ef7ce9f5111f7f423bdeaf969821", "name": "College of Architecture and Design, New Jersey Institute of Technology, Newark, NJ, USA", "category": "unknown"}, {"institution_id": "47d668887a0cfab54c6e84b418463b16", "name": "New Jersey Institute of Technology", "category": "unknown"}]}, {"author": {"author_id": "88a137317b1b44d18657a8ef0e5c4960", "name": "Jaeheung Park", "links": [{"type": "semantic_scholar", "link": "2180927943"}, {"type": "xplore", "link": "37281014000"}]}, "affiliations": [{"institution_id": "26622234bfbf9e31eb6ca7c8ddacc3c4", "name": "Seoul National University", "category": "unknown"}, {"institution_id": "47fe66dce198ee02f0f7eab4fed4b64c", "name": "Department of Intelligence and Information, Seoul National University, Seoul, Republic of Korea", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d4b989423e5d838febb54209f5a8f06", "name": "IEEE Robotics and Automation Letters", "type": "journal", "date": {"text": "2023-10", "timestamp": 1696132800, "precision": 2}, "links": [], "publisher": "IEEE", "series": "", "volume": "8"}, "peer_reviewed": true, "status": "published", "pages": "6251-6258"}], "topics": [{"name": "Contact Force"}, {"name": "Aerospace electronics"}, {"name": "Tuning"}, {"name": "Proportional-integral-derivative"}, {"name": "Bipedal Robot"}, {"name": "Computer Science"}, {"name": "Task analysis"}, {"name": "Impact Force"}, {"name": "Inductive Bias"}, {"name": "Training Policy"}, {"name": "Legged locomotion"}, {"name": "PID Controller"}, {"name": "Torque"}, {"name": "Early Contact"}, {"name": "Early Stages Of Learning"}, {"name": "torque-based control"}, {"name": "Policy Learning"}, {"name": "Tuning Parameter"}, {"name": "Proportional-integral-derivative Controller"}, {"name": "Walking Task"}, {"name": "Additional Tuning"}, {"name": "Target Velocity"}, {"name": "Humanoid Robot"}, {"name": "Reinforcement learning"}, {"name": "Agnostic Learning"}, {"name": "Real Hardware"}, {"name": "PD control"}, {"name": "Legged Robots"}, {"name": "Impact Mechanism"}, {"name": "Reality Gap"}, {"name": "Robotic Platform"}, {"name": "Deep Reinforcement Learning"}, {"name": "Joint Torque"}, {"name": "Robots"}, {"name": "Sampling Efficiency"}, {"name": "Real Robot"}, {"name": "Reference Motion"}, {"name": "Lower-level Control"}, {"name": "Reinforcement Learning Policy"}, {"name": "humanoid and bipedal locomotion"}, {"name": "Torque Control"}, {"name": "Quadruped Robot"}, {"name": "Level Of Compliance"}, {"name": "Proximal Policy Optimization"}, {"name": "Low-level Control"}], "links": [{"type": "doi.abstract", "link": "10.1109/LRA.2023.3304561", "url": "https://doi.org/10.1109/LRA.2023.3304561"}, {"type": "arxiv.abstract", "link": "2304.09434", "url": "https://arxiv.org/abs/2304.09434"}, {"type": "arxiv.pdf", "link": "2304.09434", "url": "https://arxiv.org/pdf/2304.09434.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2304-09434", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2304-09434"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2304.09434"}, {"type": "semantic_scholar.abstract", "link": "041b9d1edfe621980da89a34142868a37c381921", "url": "https://www.semanticscholar.org/paper/041b9d1edfe621980da89a34142868a37c381921"}, {"type": "corpusid", "link": "258212728"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "a7e71921b0739f2a419730cbd7c81876", "title": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning", "abstract": "Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated navigation task by using their eye gaze to modulate a 128-dimensional command signal from their webcam. The results show that our method enables successful goal navigation more often than a baseline directional interface, by learning to denoise user commands signals and provide shared autonomy assistance. We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated user commands, and find that our method improves over baseline interfaces in these domains as well. Extensive ablation experiments with simulated user commands empirically motivate each component of our method.", "authors": [{"author": {"author_id": "dec1fab1399c0f1b5759bde7f308875e", "name": "Jensen Gao", "links": [{"type": "semantic_scholar", "link": "2110482632"}, {"type": "semantic_scholar", "link": "2238154243"}, {"type": "xplore", "link": "37089448284"}]}, "affiliations": [{"institution_id": "169960bde09bf54813d1939487973bc9", "name": "Stanford University", "category": "unknown"}, {"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "828a063b62edad8579b1014f5d81cc78", "name": "Siddharth Reddy", "links": [{"type": "semantic_scholar", "link": "37372079"}, {"type": "xplore", "link": "37088506876"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "e06e180f6ddb16b17a1f8925331b98fb", "name": "Anca Dragan", "links": [{"type": "openreview", "link": "~Anca_Dragan1"}, {"type": "semantic_scholar", "link": "2064066935"}, {"type": "semantic_scholar", "link": "2745001"}, {"type": "xplore", "link": "37960625200"}]}, "affiliations": [{"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "db8d2cd3248301ddbe53e438c1f820a5", "name": "Sergey Levine", "links": [{"type": "openreview", "link": "~Sergey_Levine1"}, {"type": "semantic_scholar", "link": "1736651"}, {"type": "semantic_scholar", "link": "2249615151"}, {"type": "semantic_scholar", "link": "2257194331"}, {"type": "semantic_scholar", "link": "2279022150"}, {"type": "xplore", "link": "37085481973"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "66213907d31ae6c383b82722e0366f0c", "name": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "type": "journal", "date": {"text": "2023-10-01", "timestamp": 1696132800, "precision": 3}, "links": [], "publisher": "IEEE", "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": "7523-7530"}], "topics": [{"name": "Moon"}, {"name": "Long-term Aim"}, {"name": "Information Bottleneck"}, {"name": "Command Signal"}, {"name": "Navigation Task"}, {"name": "Simulated Task"}, {"name": "Computer Science"}, {"name": "Navigation"}, {"name": "Eye Gaze"}, {"name": "Feed-forward Network"}, {"name": "Usage Intention"}, {"name": "Latent Embedding"}, {"name": "Webcams"}, {"name": "Value Function"}, {"name": "Distribution Of Tasks"}, {"name": "Expert Users"}, {"name": "Reinforcement Learning Algorithm"}, {"name": "Priority Map"}, {"name": "Machine Learning Algorithms"}, {"name": "Recurrent Neural Network"}, {"name": "Shift In Distribution"}, {"name": "Reward Prediction Error"}, {"name": "User Study"}, {"name": "User Tasks"}, {"name": "Usual Practice"}, {"name": "Space vehicles"}, {"name": "Machine learning algorithms"}, {"name": "Training"}, {"name": "Offline Reinforcement Learning"}, {"name": "Gradient Step"}, {"name": "Long-term Intentions"}, {"name": "Reward Prediction"}, {"name": "Domain Shift"}, {"name": "Reinforcement learning"}, {"name": "Robotic Assistance"}, {"name": "Eye Contact"}, {"name": "Part Of Trajectory"}, {"name": "User Representation"}, {"name": "Ablation Experiments"}, {"name": "Long-term Purposes"}, {"name": "Reward Function"}, {"name": "Feedforward Neural Network"}, {"name": "Learning Algorithms"}, {"name": "Cold-start Problem"}], "links": [{"type": "doi.abstract", "link": "10.1109/IROS55552.2023.10341779", "url": "https://doi.org/10.1109/IROS55552.2023.10341779"}, {"type": "arxiv.abstract", "link": "2309.03839", "url": "https://arxiv.org/abs/2309.03839"}, {"type": "arxiv.pdf", "link": "2309.03839", "url": "https://arxiv.org/pdf/2309.03839.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2309-03839", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2309-03839"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2309.03839"}, {"type": "semantic_scholar.abstract", "link": "3091f9561341bfa4bd2544e6c625c2dfd19520cf", "url": "https://www.semanticscholar.org/paper/3091f9561341bfa4bd2544e6c625c2dfd19520cf"}, {"type": "corpusid", "link": "261582673"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "cd9b41c0a0772824081b126b85b08811", "title": "Reasoning with Latent Diffusion in Offline Reinforcement Learning", "abstract": "", "authors": [{"author": {"author_id": "b9ddbab408794b2203bc1c0086323653", "name": "Siddarth Venkatraman", "links": [{"type": "openreview", "link": "~Siddarth_Venkatraman1"}, {"type": "semantic_scholar", "link": "1598370068"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "ccb30393232dffc0ce00fb4c59ea21ff", "name": "Shivesh Khaitan", "links": [{"type": "openreview", "link": "~Shivesh_Khaitan1"}, {"type": "semantic_scholar", "link": "1999785394"}]}, "affiliations": [{"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}]}, {"author": {"author_id": "a2e01fc1d672fa40bee815d28c002ee7", "name": "Ravi Tej Akella", "links": [{"type": "openreview", "link": "~Ravi_Tej_Akella1"}, {"type": "semantic_scholar", "link": "38527583"}]}, "affiliations": [{"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}]}, {"author": {"author_id": "e1a842fda2859d8cddb9fda5e4864010", "name": "John Dolan", "links": [{"type": "openreview", "link": "~John_Dolan1"}, {"type": "semantic_scholar", "link": "2239202160"}]}, "affiliations": [{"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}]}, {"author": {"author_id": "8f1d23489fc487d0873bdc000ee57eaa", "name": "Jeff Schneider", "links": [{"type": "openreview", "link": "~Jeff_Schneider1"}, {"type": "semantic_scholar", "link": "2239201978"}]}, "affiliations": [{"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}, {"venue": {"venue_id": "6b00fec8c0d7598bda0e96167eaabbc1", "name": "ICLR", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Computer Science"}, {"name": "Diffusion Models"}, {"name": "Reinforcement Learning"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2309.06599", "url": "https://doi.org/10.48550/arXiv.2309.06599"}, {"type": "openreview.abstract", "link": "tGQirjzddO", "url": "https://openreview.net/forum?id=tGQirjzddO"}, {"type": "openreview.pdf", "link": "tGQirjzddO", "url": "https://openreview.net/pdf?id=tGQirjzddO"}, {"type": "arxiv.abstract", "link": "2309.06599", "url": "https://arxiv.org/abs/2309.06599"}, {"type": "arxiv.pdf", "link": "2309.06599", "url": "https://arxiv.org/pdf/2309.06599.pdf"}, {"type": "dblp.abstract", "link": "conf/iclr/VenkatramanKAD024", "url": "https://dblp.uni-trier.de/rec/conf/iclr/VenkatramanKAD024"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2309-06599", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2309-06599"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2309.06599"}, {"type": "html", "link": "https://openreview.net/forum?id=tGQirjzddO"}, {"type": "semantic_scholar.abstract", "link": "bcc5820c7a84f84347bbf1062dcf7330fe2b0870", "url": "https://www.semanticscholar.org/paper/bcc5820c7a84f84347bbf1062dcf7330fe2b0870"}, {"type": "corpusid", "link": "261706069"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "c4da78496c58d83bf9830a95c3dea273", "title": "Robust and Versatile Bipedal Jumping Control through Reinforcement Learning", "abstract": "", "authors": [{"author": {"author_id": "93cc03207aee2f4d30a097baf4d0cc7e", "name": "Zhongyu Li", "links": [{"type": "semantic_scholar", "link": "1491078398"}, {"type": "xplore", "link": "37088691308"}]}, "affiliations": [{"institution_id": "740d738029438f70cc5112151169933d", "name": "University", "category": "academia"}]}, {"author": {"author_id": "c29bd8289e233e797e229436c937a3eb", "name": "Xue Bin Peng", "links": [{"type": "semantic_scholar", "link": "32200465"}, {"type": "xplore", "link": "37086454470"}]}, "affiliations": []}, {"author": {"author_id": "b69280fba51397fdc283d1a049b765db", "name": "Pieter Abbeel", "links": [{"type": "openreview", "link": "~Pieter_Abbeel2"}, {"type": "semantic_scholar", "link": "1689992"}, {"type": "semantic_scholar", "link": "2262214983"}, {"type": "semantic_scholar", "link": "2279021699"}, {"type": "xplore", "link": "37542877900"}]}, "affiliations": [{"institution_id": "740d738029438f70cc5112151169933d", "name": "University", "category": "academia"}]}, {"author": {"author_id": "db8d2cd3248301ddbe53e438c1f820a5", "name": "Sergey Levine", "links": [{"type": "openreview", "link": "~Sergey_Levine1"}, {"type": "semantic_scholar", "link": "1736651"}, {"type": "semantic_scholar", "link": "2249615151"}, {"type": "semantic_scholar", "link": "2257194331"}, {"type": "semantic_scholar", "link": "2279022150"}, {"type": "xplore", "link": "37085481973"}]}, "affiliations": [{"institution_id": "740d738029438f70cc5112151169933d", "name": "University", "category": "academia"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "823991839546214e31e46c9c086203cb", "name": "Koushil Sreenath", "links": [{"type": "orcid", "link": "0000-0002-5346-3637"}, {"type": "semantic_scholar", "link": "144116765"}, {"type": "xplore", "link": "37563179200"}]}, "affiliations": [{"institution_id": "740d738029438f70cc5112151169933d", "name": "University", "category": "academia"}]}], "releases": [{"venue": {"venue_id": "143d75cf9bc255f3a82028eba145ceab", "name": "Robotics: Science and Systems XIX", "type": "conference", "date": {"text": "2023-07-10", "timestamp": 1688961600, "precision": 3}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Computer Science"}, {"name": "Engineering"}], "links": [{"type": "doi.abstract", "link": "10.15607/RSS.2023.XIX.052", "url": "https://doi.org/10.15607/RSS.2023.XIX.052"}, {"type": "arxiv.abstract", "link": "2302.09450", "url": "https://arxiv.org/abs/2302.09450"}, {"type": "arxiv.pdf", "link": "2302.09450", "url": "https://arxiv.org/pdf/2302.09450.pdf"}, {"type": "dblp.abstract", "link": "conf/rss/LiPALBS23", "url": "https://dblp.uni-trier.de/rec/conf/rss/LiPALBS23"}, {"type": "pdf", "link": "https://doi.org/10.15607/rss.2023.xix.052"}, {"type": "semantic_scholar.abstract", "link": "9fda8dbbc030dbf9dae798b051505756be6ffd3a", "url": "https://www.semanticscholar.org/paper/9fda8dbbc030dbf9dae798b051505756be6ffd3a"}, {"type": "corpusid", "link": "259000031"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "8930a484d8aad1cf022ec0f1a62713d5", "title": "Maximum State Entropy Exploration using Predecessor and Successor Representations", "abstract": "Animals have a developed ability to explore that aids them in important tasks such as locating food, exploring for shelter, and finding misplaced items. These exploration skills necessarily track where they have been so that they can plan for finding items with relative efficiency. Contemporary exploration algorithms often learn a less efficient exploration strategy because they either condition only on the current state or simply rely on making random open-loop exploratory moves. In this work, we propose $\\eta\\psi$-Learning, a method to learn efficient exploratory policies by conditioning on past episodic experience to make the next exploratory move. Specifically, $\\eta\\psi$-Learning learns an exploration policy that maximizes the entropy of the state visitation distribution of a single trajectory. Furthermore, we demonstrate how variants of the predecessor representation and successor representations can be combined to predict the state visitation entropy. Our experiments demonstrate the efficacy of $\\eta\\psi$-Learning to strategically explore the environment and maximize the state coverage with limited samples.", "authors": [{"author": {"author_id": "a873db153082ee433ec02f71f0aaa702", "name": "Arnav Kumar Jain", "links": [{"type": "openreview", "link": "~Arnav_Kumar_Jain2"}, {"type": "semantic_scholar", "link": "7284555"}]}, "affiliations": [{"institution_id": "099386ee291a46438cd2f3c2e05267c8", "name": "University de Montreal", "category": "academia"}]}, {"author": {"author_id": "ce65563792a1927b992b7801c86797e8", "name": "Lucas Lehnert", "links": [{"type": "openreview", "link": "~Lucas_Lehnert1"}, {"type": "semantic_scholar", "link": "2284988047"}, {"type": "semantic_scholar", "link": "39251318"}]}, "affiliations": [{"institution_id": "53ebc9c43621e396de19895e697cb413", "name": "Meta", "category": "unknown"}]}, {"author": {"author_id": "d7996e27da7f5c147d15bf0ee70b7c0c", "name": "Irina Rish", "links": [{"type": "!semantic_scholar", "link": "2064747582"}, {"type": "bio", "link": "irina-rish"}, {"type": "email.mila", "link": "irina.rish@mila.quebec"}, {"type": "mag", "link": "1653753694"}, {"type": "mag", "link": "2997858046"}, {"type": "mag", "link": "3025417811"}, {"type": "openreview", "link": "~Irina_Rish1"}, {"type": "semantic_scholar", "link": "2109771"}, {"type": "semantic_scholar", "link": "2239232896"}, {"type": "semantic_scholar", "link": "2284772297"}, {"type": "wpid_en", "link": "37584"}, {"type": "wpid_fr", "link": "37580"}, {"type": "xplore", "link": "37268997000"}]}, "affiliations": [{"institution_id": "099386ee291a46438cd2f3c2e05267c8", "name": "University de Montreal", "category": "academia"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "099386ee291a46438cd2f3c2e05267c8", "name": "University de Montreal", "category": "academia"}]}], "releases": [{"venue": {"venue_id": "2231305ebc59ffab234d656101ea6714", "name": "NeurIPS.cc/2023/Conference", "type": "conference", "date": {"text": "2023-09-21", "timestamp": 1695268800, "precision": 3}, "links": [{"type": "openreview-venue", "link": "NeurIPS.cc/2023/Conference"}], "publisher": null, "series": "", "volume": "NeurIPS 2023"}, "peer_reviewed": true, "status": "poster", "pages": null}, {"venue": {"venue_id": "729777f3ada3f43e66622a8ed8f2bf62", "name": "Advances in Neural Information Processing Systems 36  (NeurIPS 2023)", "type": "unknown", "date": {"text": "2023", "timestamp": 1672549200, "precision": 1}, "links": [], "publisher": "Curran Associates, Inc.", "series": "", "volume": "36"}, "peer_reviewed": true, "status": "published", "pages": "49991\u201350019"}], "topics": [{"name": "Successor Representation"}, {"name": "Computer Science"}, {"name": "State Visitation Distribution"}, {"name": "Maximum state entropy exploration"}, {"name": "Non-Markovian exploration"}, {"name": "Reinforcement Learning"}], "links": [{"type": "html.official", "link": "https://proceedings.neurips.cc//paper_files/paper/2023/hash/9c7900fac04a701cbed83256b76dbaa3-Abstract-Conference.html"}, {"type": "pdf.official", "link": "https://proceedings.neurips.cc/paper_files/paper/2023/file/9c7900fac04a701cbed83256b76dbaa3-Paper-Conference.pdf"}, {"type": "doi.abstract", "link": "10.48550/arXiv.2306.14808", "url": "https://doi.org/10.48550/arXiv.2306.14808"}, {"type": "openreview.abstract", "link": "inE5hW4tQ0", "url": "https://openreview.net/forum?id=inE5hW4tQ0"}, {"type": "openreview.abstract", "link": "tFsxtqGmkn", "url": "https://openreview.net/forum?id=tFsxtqGmkn"}, {"type": "openreview.pdf", "link": "inE5hW4tQ0", "url": "https://openreview.net/pdf?id=inE5hW4tQ0"}, {"type": "openreview.pdf", "link": "tFsxtqGmkn", "url": "https://openreview.net/pdf?id=tFsxtqGmkn"}, {"type": "arxiv.abstract", "link": "2306.14808", "url": "https://arxiv.org/abs/2306.14808"}, {"type": "arxiv.pdf", "link": "2306.14808", "url": "https://arxiv.org/pdf/2306.14808.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2306-14808", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2306-14808"}, {"type": "pdf", "link": "http://export.arxiv.org/pdf/2306.14808"}, {"type": "semantic_scholar.abstract", "link": "6b8e98792e4af57687939156c07b99cd12187f89", "url": "https://www.semanticscholar.org/paper/6b8e98792e4af57687939156c07b99cd12187f89"}, {"type": "corpusid", "link": "259261970"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "81709b4783324a59fd2632ee694e9071", "title": "Heterogeneous Crowd Simulation Using Parametric Reinforcement Learning", "abstract": "Agent-based synthetic crowd simulation affords the cost-effective large-scale simulation and animation of interacting digital humans. Model-based approaches have successfully generated a plethora of simulators with a variety of foundations. However, prior approaches have been based on statically defined models predicated on simplifying assumptions, limited video-based datasets, or homogeneous policies. Recent works have applied reinforcement learning to learn policies for navigation. However, these approaches may learn static homogeneous rules, are typically limited in their generalization to trained scenarios, and limited in their usability in synthetic crowd domains. In this article, we present a multi-agent reinforcement learning-based approach that learns a parametric predictive collision avoidance and steering policy. We show that training over a parameter space produces a flexible model across crowd configurations. That is, our goal-conditioned approach learns a parametric policy that affords heterogeneous synthetic crowds. We propose a model-free approach without centralization of internal agent information, control signals, or agent communication. The model is extensively evaluated. The results show policy generalization across unseen scenarios, agent parameters, and out-of-distribution parameterizations. The learned model has comparable computational performance to traditional methods. Qualitatively the model produces both expected (laminar flow, shuffling, bottleneck) and unexpected (side-stepping) emergent qualitative behaviours, and quantitatively the approach is performant across measures of movement quality.", "authors": [{"author": {"author_id": "c0c032d3b7ff5af19be908b1751a23cf", "name": "Kaidong Hu", "links": [{"type": "semantic_scholar", "link": "50726531"}, {"type": "xplore", "link": "37089746546"}]}, "affiliations": [{"institution_id": "28cadb31618609307cd35ba86b62139e", "name": "Computer Science Department, Rutgers University, New Brunswick, NJ, USA", "category": "unknown"}]}, {"author": {"author_id": "bc42a5fdcccf1367b1c656a5fcb18881", "name": "Brandon Haworth", "links": [{"type": "semantic_scholar", "link": "1398795708"}, {"type": "xplore", "link": "37085866314"}]}, "affiliations": [{"institution_id": "27b84398ca0392c69183ebb2995392d8", "name": "Department of Computer Science, University of Victoria, Victoria, BC, Canada", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "54cfd71773d34529369bc6b5d1feae82", "name": "Department of Computer Science and Operations Research, University of Montreal and a member of MILA, Montreal, Quebec, Canada", "category": "unknown"}]}, {"author": {"author_id": "d6c8126cfcf71d7b17934db72b278628", "name": "Vladimir Pavlovic", "links": [{"type": "semantic_scholar", "link": "144658464"}, {"type": "xplore", "link": "37270545400"}]}, "affiliations": [{"institution_id": "28cadb31618609307cd35ba86b62139e", "name": "Computer Science Department, Rutgers University, New Brunswick, NJ, USA", "category": "unknown"}]}, {"author": {"author_id": "d90b1bc67a3bec17efb0c978fae79d8c", "name": "Petros Faloutsos", "links": [{"type": "semantic_scholar", "link": "1737527"}, {"type": "xplore", "link": "37448800400"}]}, "affiliations": [{"institution_id": "3b48df60a957a8b1ea61583627ce412b", "name": "Department of Electrical Engineering & Computer Science, York University, Toronto, ON, Canada", "category": "unknown"}]}, {"author": {"author_id": "c97ec0b5b6cfbf7b10a9400aea00ec93", "name": "Mubbasir Kapadia", "links": [{"type": "semantic_scholar", "link": "143980996"}, {"type": "semantic_scholar", "link": "143980997"}, {"type": "xplore", "link": "37714469100"}]}, "affiliations": [{"institution_id": "28cadb31618609307cd35ba86b62139e", "name": "Computer Science Department, Rutgers University, New Brunswick, NJ, USA", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "11fd6ce3d95af931eb90417b6f3c407c", "name": "IEEE Transactions on Visualization and Computer Graphics", "type": "journal", "date": {"text": "2021-12-29", "timestamp": 1640754000, "precision": 3}, "links": [], "publisher": null, "series": "", "volume": "29"}, "peer_reviewed": true, "status": "published", "pages": null}, {"venue": {"venue_id": "088d762262ce0b6c6420e40642f96eb5", "name": "IEEE Transactions on Visualization and Computer Graphics", "type": "journal", "date": {"text": "2023-04-01", "timestamp": 1680321600, "precision": 3}, "links": [], "publisher": "IEEE", "series": "", "volume": "29"}, "peer_reviewed": true, "status": "published", "pages": "2036-2052"}], "topics": [{"name": "Learnable Parameters"}, {"name": "Medicine"}, {"name": "Emergent Behavior"}, {"name": "Multi-agent Learning"}, {"name": "Multiple Agents"}, {"name": "Long Short Term Memory"}, {"name": "Computer Science"}, {"name": "parametric policy learning"}, {"name": "Random Initialization"}, {"name": "Navigation"}, {"name": "Predictive models"}, {"name": "Neural networks"}, {"name": "Multi-agent navigation"}, {"name": "Heterogeneity Scenarios"}, {"name": "Number Of Agents"}, {"name": "Completion Time"}, {"name": "Markov Decision Process"}, {"name": "Deep Learning"}, {"name": "reinforcement learning"}, {"name": "Policy Learning"}, {"name": "Model-free Approach"}, {"name": "State Space"}, {"name": "Current Frame"}, {"name": "Neural Network"}, {"name": "Heterogeneity Of Agents"}, {"name": "State Observer"}, {"name": "Environment Configuration"}, {"name": "Part Of The State"}, {"name": "Heterogeneous Scenarios"}, {"name": "Training"}, {"name": "Computational Performance"}, {"name": "State-space"}, {"name": "Development Literature"}, {"name": "crowd simulation"}, {"name": "Training Environment"}, {"name": "Crowd Simulation"}, {"name": "Reinforcement learning"}, {"name": "Computational modeling"}, {"name": "Collision avoidance"}, {"name": "Historical Literature"}, {"name": "Long Short-term Memory"}, {"name": "Heterogeneous Agents"}, {"name": "Deep Reinforcement Learning"}, {"name": "Single Policy"}, {"name": "Reward Function"}, {"name": "Multi-agent Reinforcement Learning"}, {"name": "Private Space"}, {"name": "LSTM"}, {"name": "Personal Space"}, {"name": "Emergent Behaviour"}, {"name": "Source Model"}, {"name": "Proximal Policy Optimization"}], "links": [{"type": "doi.abstract", "link": "10.1109/TVCG.2021.3139031", "url": "https://doi.org/10.1109/TVCG.2021.3139031"}, {"type": "pubmed.abstract", "link": "34965213", "url": "https://pubmed.ncbi.nlm.nih.gov/34965213"}, {"type": "dblp.abstract", "link": "journals/tvcg/HuHBPFK23", "url": "https://dblp.uni-trier.de/rec/journals/tvcg/HuHBPFK23"}, {"type": "pdf", "link": "https://dspace.library.uvic.ca/bitstream/1828/15134/1/Haworth_IEEE_Trans._Vis._Comput._2021.pdf"}, {"type": "semantic_scholar.abstract", "link": "cadba12b17a89512e9553801b0ed2f640f832f67", "url": "https://www.semanticscholar.org/paper/cadba12b17a89512e9553801b0ed2f640f832f67"}, {"type": "corpusid", "link": "245566720"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "aee5cf2b2406f3cb6cb6e24f08d60b00", "title": "Towards Learning to Imitate from a Single Video Demonstration", "abstract": "Agents that can learn to imitate given video observation -- \\emph{without direct access to state or action information} are more applicable to learning in the natural world. However, formulating a reinforcement learning (RL) agent that facilitates this goal remains a significant challenge. We approach this challenge using contrastive training to learn a reward function comparing an agent's behaviour with a single demonstration. We use a Siamese recurrent neural network architecture to learn rewards in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we also find that the inclusion of multi-task data and additional image encoding losses improve the temporal consistency of the learned rewards and, as a result, significantly improves policy learning. We demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D and a quadruped and a humanoid in 3D. We show that our method outperforms current state-of-the-art techniques in these environments and can learn to imitate from a single video demonstration.", "authors": [{"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": []}, {"author": {"author_id": "b223f9e59d03cccdef6c12fd6c51858c", "name": "Florian Golemo", "links": [{"type": "openreview", "link": "~Florian_Golemo1"}, {"type": "semantic_scholar", "link": "2066832277"}, {"type": "semantic_scholar", "link": "2970150"}, {"type": "xplore", "link": "37086270539"}]}, "affiliations": []}, {"author": {"author_id": "d4294b54e10232088f5feff243c4e293", "name": "Christopher Pal", "links": [{"type": "bio", "link": "pal-christopher"}, {"type": "email.mila", "link": "christopher.pal@mila.quebec"}, {"type": "mag", "link": "2168619732"}, {"type": "mag", "link": "2900737546"}, {"type": "mag", "link": "2910816495"}, {"type": "mag", "link": "2923024885"}, {"type": "openreview", "link": "~Christopher_Pal1"}, {"type": "semantic_scholar", "link": "1972076"}, {"type": "semantic_scholar", "link": "2061666783"}, {"type": "semantic_scholar", "link": "2061666816"}, {"type": "semantic_scholar", "link": "2061666830"}, {"type": "semantic_scholar", "link": "2132432404"}, {"type": "semantic_scholar", "link": "2242198325"}, {"type": "semantic_scholar", "link": "2243333924"}, {"type": "semantic_scholar", "link": "2249529376"}, {"type": "semantic_scholar", "link": "2252966270"}, {"type": "semantic_scholar", "link": "2265966393"}, {"type": "semantic_scholar", "link": "2275240361"}, {"type": "semantic_scholar", "link": "2282539195"}, {"type": "semantic_scholar", "link": "2283771332"}, {"type": "semantic_scholar", "link": "2288590587"}, {"type": "semantic_scholar", "link": "2294173613"}, {"type": "semantic_scholar", "link": "2294691864"}, {"type": "semantic_scholar", "link": "2295623112"}, {"type": "semantic_scholar", "link": "2310239092"}, {"type": "semantic_scholar", "link": "98109738"}, {"type": "wpid_en", "link": "362"}, {"type": "wpid_fr", "link": "329"}, {"type": "xplore", "link": "37086726955"}, {"type": "xplore", "link": "37282879700"}]}, "affiliations": []}], "releases": [{"venue": {"venue_id": "7dbbda687595af5ebd3fa932628bc5a1", "name": "J. Mach. Learn. Res.", "type": "journal", "date": {"text": "2023", "timestamp": 1672549200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": "78:1-78:26"}, {"venue": {"venue_id": "390e40c89f9824060add53bea5507663", "name": "Journal of Machine Learning Research", "type": "unknown", "date": {"text": "2023", "timestamp": 1672549200, "precision": 1}, "links": [], "publisher": "JMLR", "series": "", "volume": "24"}, "peer_reviewed": true, "status": "published", "pages": "1\u201326"}], "topics": [{"name": "Computer Science"}, {"name": "Mathematics"}], "links": [{"type": "html.official", "link": "https://jmlr.org/papers/v24/21-1174.html"}, {"type": "pdf.official", "link": "https://jmlr.org/papers/volume24/21-1174/21-1174.pdf"}, {"type": "arxiv.abstract", "link": "1901.07186", "url": "https://arxiv.org/abs/1901.07186"}, {"type": "arxiv.pdf", "link": "1901.07186", "url": "https://arxiv.org/pdf/1901.07186.pdf"}, {"type": "dblp.abstract", "link": "journals/jmlr/BersethGP23", "url": "https://dblp.uni-trier.de/rec/journals/jmlr/BersethGP23"}, {"type": "html", "link": "http://jmlr.org/papers/v24/21-1174.html"}, {"type": "semantic_scholar.abstract", "link": "f7a73ca6b0d0491a49516676f991627a548fa1e4", "url": "https://www.semanticscholar.org/paper/f7a73ca6b0d0491a49516676f991627a548fa1e4"}, {"type": "corpusid", "link": "239338757"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "88054901fa80690fd920340ffbbfd31f", "title": "Hierarchical Reinforcement Learning for Precise Soccer Shooting Skills using a Quadrupedal Robot", "abstract": "We address the problem of enabling quadrupedal robots to perform precise shooting skills in the real world using reinforcement learning. Developing algorithms to enable a legged robot to shoot a soccer ball to a given target is a challenging problem that combines robot motion control and planning into one task. To solve this problem, we need to consider the dynamics limitation and motion stability during the control of a dynamic legged robot. Moreover, we need to consider motion planning to shoot the hard-to-model deformable ball rolling on the ground with uncertain friction to a desired location. In this paper, we propose a hierarchical framework that leverages deep reinforcement learning to train (a) a robust motion control policy that can track arbitrary motions and (b) a planning policy to decide the desired kicking motion to shoot a soccer ball to a target. We deploy the proposed framework on an A1 quadrupedal robot and enable it to accurately shoot the ball to random targets in the real world.", "authors": [{"author": {"author_id": "9245ee71cc36c8013b0d725e790995ae", "name": "Yandong Ji", "links": [{"type": "orcid", "link": "0000-0002-6948-7465"}, {"type": "semantic_scholar", "link": "2114169750"}, {"type": "xplore", "link": "37087473472"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "93cc03207aee2f4d30a097baf4d0cc7e", "name": "Zhongyu Li", "links": [{"type": "semantic_scholar", "link": "1491078398"}, {"type": "xplore", "link": "37088691308"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "b4e15fef26594cc7d57c7728b0857270", "name": "Yinan Sun", "links": [{"type": "semantic_scholar", "link": "2108940190"}, {"type": "xplore", "link": "37089661008"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "c29bd8289e233e797e229436c937a3eb", "name": "Xue Bin Peng", "links": [{"type": "semantic_scholar", "link": "32200465"}, {"type": "xplore", "link": "37086454470"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "db8d2cd3248301ddbe53e438c1f820a5", "name": "Sergey Levine", "links": [{"type": "openreview", "link": "~Sergey_Levine1"}, {"type": "semantic_scholar", "link": "1736651"}, {"type": "semantic_scholar", "link": "2249615151"}, {"type": "semantic_scholar", "link": "2257194331"}, {"type": "semantic_scholar", "link": "2279022150"}, {"type": "xplore", "link": "37085481973"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "0b1847fd61d72cd82f9a3130f23c9c94", "name": "Universit\u00e9 de Montr\u00e9al, Mila", "category": "unknown"}, {"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}]}, {"author": {"author_id": "823991839546214e31e46c9c086203cb", "name": "Koushil Sreenath", "links": [{"type": "orcid", "link": "0000-0002-5346-3637"}, {"type": "semantic_scholar", "link": "144116765"}, {"type": "xplore", "link": "37563179200"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "2238a30b81de098bf9065974d140d407", "name": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "type": "journal", "date": {"text": "2022-10-23", "timestamp": 1666497600, "precision": 3}, "links": [], "publisher": "IEEE", "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": "1479-1486"}, {"venue": {"venue_id": "5ae6851417e86932e4d5aefc87a91ff4", "name": "IROS", "type": "journal", "date": {"text": "2022", "timestamp": 1641013200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": "1479-1486"}], "topics": [{"name": "Policy Planning"}, {"name": "Motor Control"}, {"name": "Path Planning"}, {"name": "Ball Position"}, {"name": "Dynamics"}, {"name": "Robot motion"}, {"name": "Time Step"}, {"name": "Bezier Curve"}, {"name": "Computer Science"}, {"name": "Random Force"}, {"name": "Legged locomotion"}, {"name": "Simulation Training"}, {"name": "Time Span"}, {"name": "Low-pass"}, {"name": "Soccer Shooting"}, {"name": "Stance Leg"}, {"name": "Target tracking"}, {"name": "Quadrupedal robots"}, {"name": "Robot Motion"}, {"name": "Robust Policy"}, {"name": "Hierarchical Framework"}, {"name": "Humanoid Robot"}, {"name": "Reinforcement learning"}, {"name": "Model-based Methods"}, {"name": "Past Conditions"}, {"name": "Legged Robots"}, {"name": "Deep Reinforcement Learning"}, {"name": "Robotic Arm"}, {"name": "Motion In Order"}, {"name": "Real Robot"}, {"name": "Model-free Reinforcement Learning"}, {"name": "Stable Motion"}, {"name": "Soccer Ball"}, {"name": "End-effector Position"}, {"name": "Engineering"}, {"name": "Planning"}, {"name": "Quadruped Robot"}, {"name": "Robot State"}, {"name": "Hierarchical Reinforcement Learning"}], "links": [{"type": "doi.abstract", "link": "10.1109/IROS47612.2022.9981984", "url": "https://doi.org/10.1109/IROS47612.2022.9981984"}, {"type": "arxiv.abstract", "link": "2208.01160", "url": "https://arxiv.org/abs/2208.01160"}, {"type": "arxiv.pdf", "link": "2208.01160", "url": "https://arxiv.org/pdf/2208.01160.pdf"}, {"type": "dblp.abstract", "link": "conf/iros/JiLSPLBS22", "url": "https://dblp.uni-trier.de/rec/conf/iros/JiLSPLBS22"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2208-01160", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2208-01160"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2208.01160"}, {"type": "semantic_scholar.abstract", "link": "844c69fd25f016a7ba9b64255c65e15c7d1f6fc4", "url": "https://www.semanticscholar.org/paper/844c69fd25f016a7ba9b64255c65e15c7d1f6fc4"}, {"type": "corpusid", "link": "251253136"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "c1241147dab9dd2fd978aa49cdbd82b3", "title": "ASHA: Assistive Teleoperation via Human-in-the-Loop Reinforcement Learning", "abstract": "Building assistive interfaces for controlling robots through arbitrary, high-dimensional, noisy inputs (e.g., webcam images of eye gaze) can be challenging, especially when it involves inferring the user's desired action in the absence of a natural \u2018default\u2019 interface. Reinforcement learning from online user feedback on the system's performance presents a natural solution to this problem, and enables the interface to adapt to individual users. However, this approach tends to require a large amount of human-in-the-loop training data, especially when feedback is sparse. We propose a hierarchical solution that learns efficiently from sparse user feedback: we use offline pre-training to acquire a latent embedding space of useful, high-level robot behaviors, which, in turn, enables the system to focus on using online user feedback to learn a mapping from user inputs to desired high-level behaviors. The key insight is that access to a pre-trained policy enables the system to learn more from sparse rewards than a na\u00efve RL algorithm: using the pre-trained policy, the system can make use of successful task executions to relabel, in hindsight, what the user actually meant to do during unsuccessful executions. We evaluate our method primarily through a user study with 12 participants who perform tasks in three simulated robotic manipulation domains using a webcam and their eye gaze: flipping light switches, opening a shelf door to reach objects inside, and rotating a valve. The results show that our method successfully learns to map 128-dimensional gaze features to 7-dimensional joint torques from sparse rewards in under 10 minutes of online training, and seamlessly helps users who employ different gaze strategies, while adapting to distributional shift in webcam inputs, tasks, and environments", "authors": [{"author": {"author_id": "b1ea6f2cc24ebbb820f4a8eade6a8384", "name": "Sean Chen", "links": [{"type": "semantic_scholar", "link": "2141325983"}, {"type": "xplore", "link": "37089447557"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}]}, {"author": {"author_id": "dec1fab1399c0f1b5759bde7f308875e", "name": "Jensen Gao", "links": [{"type": "semantic_scholar", "link": "2110482632"}, {"type": "semantic_scholar", "link": "2238154243"}, {"type": "xplore", "link": "37089448284"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}]}, {"author": {"author_id": "828a063b62edad8579b1014f5d81cc78", "name": "Siddharth Reddy", "links": [{"type": "semantic_scholar", "link": "37372079"}, {"type": "xplore", "link": "37088506876"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "e06e180f6ddb16b17a1f8925331b98fb", "name": "Anca Dragan", "links": [{"type": "openreview", "link": "~Anca_Dragan1"}, {"type": "semantic_scholar", "link": "2064066935"}, {"type": "semantic_scholar", "link": "2745001"}, {"type": "xplore", "link": "37960625200"}]}, "affiliations": [{"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "db8d2cd3248301ddbe53e438c1f820a5", "name": "Sergey Levine", "links": [{"type": "openreview", "link": "~Sergey_Levine1"}, {"type": "semantic_scholar", "link": "1736651"}, {"type": "semantic_scholar", "link": "2249615151"}, {"type": "semantic_scholar", "link": "2257194331"}, {"type": "semantic_scholar", "link": "2279022150"}, {"type": "xplore", "link": "37085481973"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "11413c609c9b35b5dc3f92e287543e6c", "name": "2022 International Conference on Robotics and Automation (ICRA)", "type": "journal", "date": {"text": "2022-05-23", "timestamp": 1653278400, "precision": 3}, "links": [], "publisher": "IEEE", "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": "7505-7512"}], "topics": [{"name": "User Control"}, {"name": "User Feedback"}, {"name": "End Of Episode"}, {"name": "Optimal Policy"}, {"name": "Computer Science"}, {"name": "POMDP"}, {"name": "Eye Gaze"}, {"name": "Uniform Distribution"}, {"name": "System Performance"}, {"name": "Noisy Input"}, {"name": "Latent Embedding"}, {"name": "User Input"}, {"name": "Webcams"}, {"name": "Robot Manipulator"}, {"name": "Simulation Domain"}, {"name": "Embedding Space"}, {"name": "Distribution Of Tasks"}, {"name": "Equation Modeling"}, {"name": "Latent Variable Model"}, {"name": "Reinforcement Learning Algorithm"}, {"name": "Task Success"}, {"name": "End Of Each Episode"}, {"name": "Partially Observable Markov Decision Process"}, {"name": "User Study"}, {"name": "Training"}, {"name": "System performance"}, {"name": "Task Shifting"}, {"name": "Latent Space"}, {"name": "Individual Users"}, {"name": "Reinforcement learning"}, {"name": "Eye Contact"}, {"name": "Human in the loop"}, {"name": "Hindsight"}, {"name": "Valves"}, {"name": "Pre-training Tasks"}, {"name": "Successional Trajectories"}, {"name": "Training data"}, {"name": "Domain Switching"}, {"name": "Minutes Of Training"}, {"name": "Online Users"}, {"name": "Input Encoding"}], "links": [{"type": "doi.abstract", "link": "10.1109/icra46639.2022.9812442", "url": "https://doi.org/10.1109/icra46639.2022.9812442"}, {"type": "arxiv.abstract", "link": "2202.02465", "url": "https://arxiv.org/abs/2202.02465"}, {"type": "arxiv.pdf", "link": "2202.02465", "url": "https://arxiv.org/pdf/2202.02465.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2202-02465", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2202-02465"}, {"type": "pdf", "link": "http://export.arxiv.org/pdf/2202.02465"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2202.02465"}, {"type": "semantic_scholar.abstract", "link": "a1189ba5d86d32bc5fecd32ee905f8ff4767cbdb", "url": "https://www.semanticscholar.org/paper/a1189ba5d86d32bc5fecd32ee905f8ff4767cbdb"}, {"type": "corpusid", "link": "237263044"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}]
