[{"paper_id": "a4dcc843c634b539da4c7c2644791032", "title": "Intelligent Switching for Reset-Free RL", "abstract": "In the real world, the strong episode resetting mechanisms that are needed to train agents in simulation are unavailable. The \\textit{resetting} assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional handcrafted mechanisms or human interventions. Recent work aims to train agents (\\textit{forward}) with learned resets by constructing a second (\\textit{backward}) agent that returns the forward agent to the initial state. We find that the termination and timing of the transitions between these two agents are crucial for algorithm success. With this in mind, we create a new algorithm, Reset Free RL with Intelligently Switching Controller (RISC) which intelligently switches between the two agents based on the agent's confidence in achieving its current goal. Our new method achieves state-of-the-art performance on several challenging environments for reset-free RL.", "authors": [{"author": {"author_id": "dc206682d3ddebdc07d73cdb8593b24f", "name": "Darshan Patil", "links": [{"type": "openreview", "link": "~Darshan_Patil1"}, {"type": "semantic_scholar", "link": "2111849117"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "9615ba8643982f3eb958389ddfc88e2b", "name": "Janarthanan Rajendran", "links": [{"type": "openreview", "link": "~Janarthanan_Rajendran1"}, {"type": "openreview", "link": "~Janarthanan_Rajendran2"}, {"type": "semantic_scholar", "link": "10197529"}]}, "affiliations": [{"institution_id": "48314fb4f8bc3694a5542910d319f0b8", "name": "Dalhousie University", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "0a4595cdfa4e6d9d22cc74f6a6a2666f", "name": "Canada CIFAR AI Chair", "category": "unknown"}, {"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "95533338b3953432f8ec4743f02341b3", "name": "Sarath Chandar", "links": [{"type": "!semantic_scholar", "link": "1783528"}, {"type": "!semantic_scholar", "link": "92768738"}, {"type": "bio", "link": "sarath-chandar"}, {"type": "email.mila", "link": "sarath.chandar@mila.quebec"}, {"type": "mag", "link": "2044221253"}, {"type": "mag", "link": "2616279507"}, {"type": "mag", "link": "2994242150"}, {"type": "mag", "link": "3035505426"}, {"type": "openreview", "link": "~Sarath_Chandar1"}, {"type": "semantic_scholar", "link": "123607932"}, {"type": "semantic_scholar", "link": "144631588"}, {"type": "wpid_en", "link": "1777"}, {"type": "wpid_fr", "link": "1780"}, {"type": "xplore", "link": "37085932450"}, {"type": "xplore", "link": "37086254634"}, {"type": "xplore", "link": "37087078794"}]}, "affiliations": [{"institution_id": "0a4595cdfa4e6d9d22cc74f6a6a2666f", "name": "Canada CIFAR AI Chair", "category": "unknown"}, {"institution_id": "316e18a8da455bed7b7e9221fd7b6ae9", "name": "\u00c9cole Polytechnique de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}, {"venue": {"venue_id": "213ea6e666cc9ccdc524f4a3ce35e592", "name": "International Conference on Learning Representations", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": "abs/2405.01684"}, "peer_reviewed": true, "status": "published", "pages": null}, {"venue": {"venue_id": "6b00fec8c0d7598bda0e96167eaabbc1", "name": "ICLR", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Computer Science"}, {"name": "Reset-Free RL"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2405.01684", "url": "https://doi.org/10.48550/arXiv.2405.01684"}, {"type": "openreview.abstract", "link": "Nq45xeghcL", "url": "https://openreview.net/forum?id=Nq45xeghcL"}, {"type": "openreview.pdf", "link": "Nq45xeghcL", "url": "https://openreview.net/pdf?id=Nq45xeghcL"}, {"type": "arxiv.abstract", "link": "2405.01684", "url": "https://arxiv.org/abs/2405.01684"}, {"type": "arxiv.pdf", "link": "2405.01684", "url": "https://arxiv.org/pdf/2405.01684.pdf"}, {"type": "dblp.abstract", "link": "conf/iclr/PatilRBC24", "url": "https://dblp.uni-trier.de/rec/conf/iclr/PatilRBC24"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2405-01684", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2405-01684"}, {"type": "html", "link": "https://openreview.net/forum?id=Nq45xeghcL"}, {"type": "semantic_scholar.abstract", "link": "1463eb69e412ee1e44bfcb75e666cfc1dc0ba3fc", "url": "https://www.semanticscholar.org/paper/1463eb69e412ee1e44bfcb75e666cfc1dc0ba3fc"}, {"type": "semantic_scholar.abstract", "link": "906163e86c50998e2ab2223ed2f897f49175cc2d", "url": "https://www.semanticscholar.org/paper/906163e86c50998e2ab2223ed2f897f49175cc2d"}, {"type": "corpusid", "link": "269587774"}, {"type": "corpusid", "link": "270370048"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "b9c8f2cad469f240448d67ad7c0db3d2", "title": "Closing the Gap between TD Learning and Supervised Learning - A Generalisation Point of View", "abstract": "Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching and generalisation reveals why we should not expect SL-based RL methods to perform stitching, even in the limit of large datasets and models. Based on this analysis, we construct new datasets to explicitly test for this property, revealing that SL-based methods lack this stitching property and hence fail to perform combinatorial generalization. Nonetheless, the connection between stitching and combinatorial generalisation also suggests a simple remedy for improving generalisation in SL: data augmentation. We propose a temporal data augmentation and demonstrate that adding it to SL-based methods enables them to successfully complete tasks not seen together during training. On a high level, this connection illustrates the importance of combinatorial generalization for data efficiency in time-series data beyond tasks beyond RL, like audio, video, or text.", "authors": [{"author": {"author_id": "e23ef866bc9388321417ac2b3e4d19d2", "name": "Raj Ghugare", "links": [{"type": "openreview", "link": "~Raj_Ghugare1"}, {"type": "semantic_scholar", "link": "2185406398"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "a8fbe3b4d9281d30e20479256400ebd8", "name": "Matthieu Geist", "links": [{"type": "openreview", "link": "~Matthieu_Geist1"}, {"type": "semantic_scholar", "link": "1737555"}, {"type": "semantic_scholar", "link": "2253609155"}, {"type": "semantic_scholar", "link": "2269464568"}]}, "affiliations": []}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "92dd61a3c7714571a096058c04c40646", "name": "Benjamin Eysenbach", "links": [{"type": "openreview", "link": "~Benjamin_Eysenbach1"}, {"type": "semantic_scholar", "link": "8140754"}]}, "affiliations": [{"institution_id": "7ec86b4c6bba99408f0ee04995bfa897", "name": "Princeton University", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}, {"venue": {"venue_id": "6b00fec8c0d7598bda0e96167eaabbc1", "name": "ICLR", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Computer Science"}, {"name": "data augmentation"}, {"name": "reinforcement learning"}, {"name": "stitching"}, {"name": "reinforcement learning decision transformers"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2401.11237", "url": "https://doi.org/10.48550/arXiv.2401.11237"}, {"type": "openreview.abstract", "link": "1SJZVCahQW", "url": "https://openreview.net/forum?id=1SJZVCahQW"}, {"type": "openreview.abstract", "link": "S1aOVW3EWv", "url": "https://openreview.net/forum?id=S1aOVW3EWv"}, {"type": "openreview.abstract", "link": "qg5JENs0N4", "url": "https://openreview.net/forum?id=qg5JENs0N4"}, {"type": "openreview.pdf", "link": "1SJZVCahQW", "url": "https://openreview.net/pdf?id=1SJZVCahQW"}, {"type": "openreview.pdf", "link": "S1aOVW3EWv", "url": "https://openreview.net/pdf?id=S1aOVW3EWv"}, {"type": "openreview.pdf", "link": "qg5JENs0N4", "url": "https://openreview.net/pdf?id=qg5JENs0N4"}, {"type": "arxiv.abstract", "link": "2401.11237", "url": "https://arxiv.org/abs/2401.11237"}, {"type": "arxiv.pdf", "link": "2401.11237", "url": "https://arxiv.org/pdf/2401.11237.pdf"}, {"type": "dblp.abstract", "link": "conf/iclr/GhugareGBE24", "url": "https://dblp.uni-trier.de/rec/conf/iclr/GhugareGBE24"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2401-11237", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2401-11237"}, {"type": "html", "link": "https://openreview.net/forum?id=qg5JENs0N4"}, {"type": "semantic_scholar.abstract", "link": "9cc6151bad5d027b00ff6931e2ab7012beffa841", "url": "https://www.semanticscholar.org/paper/9cc6151bad5d027b00ff6931e2ab7012beffa841"}, {"type": "corpusid", "link": "267068467"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "d3da82b02323ec64ced8fb80956eca32", "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers", "abstract": "", "authors": [{"author": {"author_id": "e23ef866bc9388321417ac2b3e4d19d2", "name": "Raj Ghugare", "links": [{"type": "openreview", "link": "~Raj_Ghugare1"}, {"type": "semantic_scholar", "link": "2185406398"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "2e7644e4488777eb735a190201699cd0", "name": "Mila - Quebec AI Institute", "category": "unknown"}]}, {"author": {"author_id": "9e2eebd82ff2df791c97661885142f8d", "name": "Santiago Miret", "links": [{"type": "openreview", "link": "~Santiago_Miret1"}, {"type": "semantic_scholar", "link": "2237988122"}, {"type": "semantic_scholar", "link": "2259929505"}, {"type": "semantic_scholar", "link": "2273773286"}, {"type": "semantic_scholar", "link": "51895312"}]}, "affiliations": [{"institution_id": "6ec952c1c04eb0612c33c2f19a4c99b7", "name": "Intel Labs", "category": "unknown"}]}, {"author": {"author_id": "d66c37481612a532aff2122c259f0740", "name": "Adriana Hugessen", "links": [{"type": "openreview", "link": "~Adriana_Hugessen1"}, {"type": "semantic_scholar", "link": "2253652590"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "2e7644e4488777eb735a190201699cd0", "name": "Mila - Quebec AI Institute", "category": "unknown"}]}, {"author": {"author_id": "87ac14bb4d4583abfc322f1fc3c0eaa4", "name": "Mariano Phielipp", "links": [{"type": "openreview", "link": "~Mariano_Phielipp2"}, {"type": "semantic_scholar", "link": "2253655183"}, {"type": "semantic_scholar", "link": "2482400"}]}, "affiliations": [{"institution_id": "6ec952c1c04eb0612c33c2f19a4c99b7", "name": "Intel Labs", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "2e7644e4488777eb735a190201699cd0", "name": "Mila - Quebec AI Institute", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}], "topics": [{"name": "Physics"}, {"name": "chemistry"}, {"name": "Computer Science"}, {"name": "molecular docking"}, {"name": "reinforcement learning"}, {"name": "language models"}, {"name": "pytdc"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2310.02902", "url": "https://doi.org/10.48550/arXiv.2310.02902"}, {"type": "openreview.abstract", "link": "O8mZO2ri33", "url": "https://openreview.net/forum?id=O8mZO2ri33"}, {"type": "openreview.abstract", "link": "nqlymMx42E", "url": "https://openreview.net/forum?id=nqlymMx42E"}, {"type": "openreview.pdf", "link": "O8mZO2ri33", "url": "https://openreview.net/pdf?id=O8mZO2ri33"}, {"type": "openreview.pdf", "link": "nqlymMx42E", "url": "https://openreview.net/pdf?id=nqlymMx42E"}, {"type": "arxiv.abstract", "link": "2310.02902", "url": "https://arxiv.org/abs/2310.02902"}, {"type": "arxiv.pdf", "link": "2310.02902", "url": "https://arxiv.org/pdf/2310.02902.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2310-02902", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2310-02902"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2310.02902"}, {"type": "semantic_scholar.abstract", "link": "8923aec569a13f94148e3e90a94c68730f6ad03d", "url": "https://www.semanticscholar.org/paper/8923aec569a13f94148e3e90a94c68730f6ad03d"}, {"type": "corpusid", "link": "263620293"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "85fe32ed5c3365ddbea06d54544182b1", "title": "Improving Intrinsic Exploration by Creating Stationary Objectives", "abstract": "", "authors": [{"author": {"author_id": "9e8e03d8b383bafb594acdf3b9b91241", "name": "Roger Creus Castanyer", "links": [{"type": "openreview", "link": "~Roger_Creus_Castanyer1"}, {"type": "semantic_scholar", "link": "2053524695"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "3b267db4aea551ea093661ef7519c1ec", "name": "Mila Qu\u00e9bec AI Institute", "category": "academia"}]}, {"author": {"author_id": "8a36f7d2afa669eb753077880232afb5", "name": "Joshua Romoff", "links": [{"type": "openreview", "link": "~Joshua_Romoff1"}, {"type": "semantic_scholar", "link": "8365320"}]}, "affiliations": []}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "3b267db4aea551ea093661ef7519c1ec", "name": "Mila Qu\u00e9bec AI Institute", "category": "academia"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}, {"venue": {"venue_id": "6b00fec8c0d7598bda0e96167eaabbc1", "name": "ICLR", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Stationarity"}, {"name": "Computer Science"}, {"name": "Exploration"}, {"name": "Reinforcement Learning"}, {"name": "Intrinsic Rewards"}, {"name": "Intrinsic Objectives"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2310.18144", "url": "https://doi.org/10.48550/arXiv.2310.18144"}, {"type": "openreview.abstract", "link": "V7Ao0FdXEn", "url": "https://openreview.net/forum?id=V7Ao0FdXEn"}, {"type": "openreview.abstract", "link": "YbZxT0SON4", "url": "https://openreview.net/forum?id=YbZxT0SON4"}, {"type": "openreview.pdf", "link": "V7Ao0FdXEn", "url": "https://openreview.net/pdf?id=V7Ao0FdXEn"}, {"type": "openreview.pdf", "link": "YbZxT0SON4", "url": "https://openreview.net/pdf?id=YbZxT0SON4"}, {"type": "arxiv.abstract", "link": "2310.18144", "url": "https://arxiv.org/abs/2310.18144"}, {"type": "arxiv.pdf", "link": "2310.18144", "url": "https://arxiv.org/pdf/2310.18144.pdf"}, {"type": "dblp.abstract", "link": "conf/iclr/CastanyerRB24", "url": "https://dblp.uni-trier.de/rec/conf/iclr/CastanyerRB24"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2310-18144", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2310-18144"}, {"type": "html", "link": "https://openreview.net/forum?id=YbZxT0SON4"}, {"type": "semantic_scholar.abstract", "link": "99f9198f9d7ae72d09c9ddc7a8451f119dfeab84", "url": "https://www.semanticscholar.org/paper/99f9198f9d7ae72d09c9ddc7a8451f119dfeab84"}, {"type": "corpusid", "link": "264555396"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "ad2e4e26a1db7eb96aadd65589ef313f", "title": "Torque-Based Deep Reinforcement Learning for Task-and-Robot Agnostic Learning on Bipedal Robots Using Sim-to-Real Transfer", "abstract": "In this letter, we review the question of which action space is best suited for controlling a real biped robot in combination with Sim2Real training. Position control has been popular as it has been shown to be more sample efficient and intuitive to combine with other planning algorithms. However, for position control, gain tuning is required to achieve the best possible policy performance. We show that, instead, using a torque-based action space enables task-and-robot agnostic learning with less parameter tuning and mitigates the sim-to-reality gap by taking advantage of torque control's inherent compliance. Also, we accelerate the torque-based-policy training process by pre-training the policy to remain upright by compensating for gravity. The letter showcases the first successful sim-to-real transfer of a torque-based deep reinforcement learning policy on a real human-sized biped robot.", "authors": [{"author": {"author_id": "bf1cd394cc29bbc5a0b5f98c777f785d", "name": "Donghyeon Kim", "links": [{"type": "semantic_scholar", "link": "1799288522"}, {"type": "xplore", "link": "37087324205"}]}, "affiliations": [{"institution_id": "47fe66dce198ee02f0f7eab4fed4b64c", "name": "Department of Intelligence and Information, Seoul National University, Seoul, Republic of Korea", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "45223f65c0fc88e938aa08c86ddc3973", "name": "Universit\u00e9 de Montr\u00e9al, Quebec, Canada", "category": "unknown"}, {"institution_id": "4cfe82f20f6016691ccda3e248335bd9", "name": "Canada", "category": "unknown"}]}, {"author": {"author_id": "999a07a3603e51fab1b05107bab999f7", "name": "Mathew Schwartz", "links": [{"type": "semantic_scholar", "link": "2980585"}, {"type": "xplore", "link": "37085580327"}]}, "affiliations": [{"institution_id": "3ca9ef7ce9f5111f7f423bdeaf969821", "name": "College of Architecture and Design, New Jersey Institute of Technology, Newark, NJ, USA", "category": "unknown"}, {"institution_id": "47d668887a0cfab54c6e84b418463b16", "name": "New Jersey Institute of Technology", "category": "unknown"}]}, {"author": {"author_id": "88a137317b1b44d18657a8ef0e5c4960", "name": "Jaeheung Park", "links": [{"type": "semantic_scholar", "link": "2180927943"}, {"type": "xplore", "link": "37281014000"}]}, "affiliations": [{"institution_id": "26622234bfbf9e31eb6ca7c8ddacc3c4", "name": "Seoul National University", "category": "unknown"}, {"institution_id": "47fe66dce198ee02f0f7eab4fed4b64c", "name": "Department of Intelligence and Information, Seoul National University, Seoul, Republic of Korea", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d4b989423e5d838febb54209f5a8f06", "name": "IEEE Robotics and Automation Letters", "type": "journal", "date": {"text": "2023-10", "timestamp": 1696132800, "precision": 2}, "links": [], "publisher": "IEEE", "series": "", "volume": "8"}, "peer_reviewed": true, "status": "published", "pages": "6251-6258"}], "topics": [{"name": "Contact Force"}, {"name": "Aerospace electronics"}, {"name": "Tuning"}, {"name": "Proportional-integral-derivative"}, {"name": "Bipedal Robot"}, {"name": "Computer Science"}, {"name": "Task analysis"}, {"name": "Impact Force"}, {"name": "Inductive Bias"}, {"name": "Training Policy"}, {"name": "Legged locomotion"}, {"name": "PID Controller"}, {"name": "Torque"}, {"name": "Early Contact"}, {"name": "Early Stages Of Learning"}, {"name": "torque-based control"}, {"name": "Policy Learning"}, {"name": "Tuning Parameter"}, {"name": "Proportional-integral-derivative Controller"}, {"name": "Walking Task"}, {"name": "Additional Tuning"}, {"name": "Target Velocity"}, {"name": "Humanoid Robot"}, {"name": "Reinforcement learning"}, {"name": "Agnostic Learning"}, {"name": "Real Hardware"}, {"name": "PD control"}, {"name": "Legged Robots"}, {"name": "Impact Mechanism"}, {"name": "Reality Gap"}, {"name": "Robotic Platform"}, {"name": "Deep Reinforcement Learning"}, {"name": "Joint Torque"}, {"name": "Robots"}, {"name": "Sampling Efficiency"}, {"name": "Real Robot"}, {"name": "Reference Motion"}, {"name": "Lower-level Control"}, {"name": "Reinforcement Learning Policy"}, {"name": "humanoid and bipedal locomotion"}, {"name": "Torque Control"}, {"name": "Quadruped Robot"}, {"name": "Level Of Compliance"}, {"name": "Proximal Policy Optimization"}, {"name": "Low-level Control"}], "links": [{"type": "doi.abstract", "link": "10.1109/LRA.2023.3304561", "url": "https://doi.org/10.1109/LRA.2023.3304561"}, {"type": "arxiv.abstract", "link": "2304.09434", "url": "https://arxiv.org/abs/2304.09434"}, {"type": "arxiv.pdf", "link": "2304.09434", "url": "https://arxiv.org/pdf/2304.09434.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2304-09434", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2304-09434"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2304.09434"}, {"type": "semantic_scholar.abstract", "link": "041b9d1edfe621980da89a34142868a37c381921", "url": "https://www.semanticscholar.org/paper/041b9d1edfe621980da89a34142868a37c381921"}, {"type": "corpusid", "link": "258212728"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "a7e71921b0739f2a419730cbd7c81876", "title": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning", "abstract": "Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated navigation task by using their eye gaze to modulate a 128-dimensional command signal from their webcam. The results show that our method enables successful goal navigation more often than a baseline directional interface, by learning to denoise user commands signals and provide shared autonomy assistance. We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated user commands, and find that our method improves over baseline interfaces in these domains as well. Extensive ablation experiments with simulated user commands empirically motivate each component of our method.", "authors": [{"author": {"author_id": "dec1fab1399c0f1b5759bde7f308875e", "name": "Jensen Gao", "links": [{"type": "semantic_scholar", "link": "2110482632"}, {"type": "semantic_scholar", "link": "2238154243"}, {"type": "xplore", "link": "37089448284"}]}, "affiliations": [{"institution_id": "169960bde09bf54813d1939487973bc9", "name": "Stanford University", "category": "unknown"}, {"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "828a063b62edad8579b1014f5d81cc78", "name": "Siddharth Reddy", "links": [{"type": "semantic_scholar", "link": "37372079"}, {"type": "xplore", "link": "37088506876"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "e06e180f6ddb16b17a1f8925331b98fb", "name": "Anca Dragan", "links": [{"type": "openreview", "link": "~Anca_Dragan1"}, {"type": "semantic_scholar", "link": "2064066935"}, {"type": "semantic_scholar", "link": "2745001"}, {"type": "xplore", "link": "37960625200"}]}, "affiliations": [{"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}, {"author": {"author_id": "db8d2cd3248301ddbe53e438c1f820a5", "name": "Sergey Levine", "links": [{"type": "openreview", "link": "~Sergey_Levine1"}, {"type": "semantic_scholar", "link": "1736651"}, {"type": "semantic_scholar", "link": "2249615151"}, {"type": "semantic_scholar", "link": "2257194331"}, {"type": "semantic_scholar", "link": "2279022150"}, {"type": "xplore", "link": "37085481973"}]}, "affiliations": [{"institution_id": "47114d77571f2b96dcc886ae0e72de12", "name": "University of California", "category": "unknown"}, {"institution_id": "593919b0c9054ede3d9383dd78adbc8a", "name": "University of California, Berkeley", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "66213907d31ae6c383b82722e0366f0c", "name": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "type": "journal", "date": {"text": "2023-10-01", "timestamp": 1696132800, "precision": 3}, "links": [], "publisher": "IEEE", "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": "7523-7530"}], "topics": [{"name": "Moon"}, {"name": "Long-term Aim"}, {"name": "Information Bottleneck"}, {"name": "Command Signal"}, {"name": "Navigation Task"}, {"name": "Simulated Task"}, {"name": "Computer Science"}, {"name": "Navigation"}, {"name": "Eye Gaze"}, {"name": "Feed-forward Network"}, {"name": "Usage Intention"}, {"name": "Latent Embedding"}, {"name": "Webcams"}, {"name": "Value Function"}, {"name": "Distribution Of Tasks"}, {"name": "Expert Users"}, {"name": "Reinforcement Learning Algorithm"}, {"name": "Priority Map"}, {"name": "Machine Learning Algorithms"}, {"name": "Recurrent Neural Network"}, {"name": "Shift In Distribution"}, {"name": "Reward Prediction Error"}, {"name": "User Study"}, {"name": "User Tasks"}, {"name": "Usual Practice"}, {"name": "Space vehicles"}, {"name": "Machine learning algorithms"}, {"name": "Training"}, {"name": "Offline Reinforcement Learning"}, {"name": "Gradient Step"}, {"name": "Long-term Intentions"}, {"name": "Reward Prediction"}, {"name": "Domain Shift"}, {"name": "Reinforcement learning"}, {"name": "Robotic Assistance"}, {"name": "Eye Contact"}, {"name": "Part Of Trajectory"}, {"name": "User Representation"}, {"name": "Ablation Experiments"}, {"name": "Long-term Purposes"}, {"name": "Reward Function"}, {"name": "Feedforward Neural Network"}, {"name": "Learning Algorithms"}, {"name": "Cold-start Problem"}], "links": [{"type": "doi.abstract", "link": "10.1109/IROS55552.2023.10341779", "url": "https://doi.org/10.1109/IROS55552.2023.10341779"}, {"type": "arxiv.abstract", "link": "2309.03839", "url": "https://arxiv.org/abs/2309.03839"}, {"type": "arxiv.pdf", "link": "2309.03839", "url": "https://arxiv.org/pdf/2309.03839.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2309-03839", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2309-03839"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2309.03839"}, {"type": "semantic_scholar.abstract", "link": "3091f9561341bfa4bd2544e6c625c2dfd19520cf", "url": "https://www.semanticscholar.org/paper/3091f9561341bfa4bd2544e6c625c2dfd19520cf"}, {"type": "corpusid", "link": "261582673"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "cd9b41c0a0772824081b126b85b08811", "title": "Reasoning with Latent Diffusion in Offline Reinforcement Learning", "abstract": "", "authors": [{"author": {"author_id": "b9ddbab408794b2203bc1c0086323653", "name": "Siddarth Venkatraman", "links": [{"type": "openreview", "link": "~Siddarth_Venkatraman1"}, {"type": "semantic_scholar", "link": "1598370068"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "ccb30393232dffc0ce00fb4c59ea21ff", "name": "Shivesh Khaitan", "links": [{"type": "openreview", "link": "~Shivesh_Khaitan1"}, {"type": "semantic_scholar", "link": "1999785394"}]}, "affiliations": [{"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}]}, {"author": {"author_id": "a2e01fc1d672fa40bee815d28c002ee7", "name": "Ravi Tej Akella", "links": [{"type": "openreview", "link": "~Ravi_Tej_Akella1"}, {"type": "semantic_scholar", "link": "38527583"}]}, "affiliations": [{"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}, {"institution_id": "5dccef41870354e16fd55fbde2db5aba", "name": "Equal Contribution", "category": "unknown"}]}, {"author": {"author_id": "e1a842fda2859d8cddb9fda5e4864010", "name": "John Dolan", "links": [{"type": "openreview", "link": "~John_Dolan1"}, {"type": "semantic_scholar", "link": "2239202160"}]}, "affiliations": [{"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}]}, {"author": {"author_id": "8f1d23489fc487d0873bdc000ee57eaa", "name": "Jeff Schneider", "links": [{"type": "openreview", "link": "~Jeff_Schneider1"}, {"type": "semantic_scholar", "link": "2239201978"}]}, "affiliations": [{"institution_id": "5cffccc10fbdaa58bd9db5868356af9a", "name": "Carnegie Mellon University", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}, {"institution_id": "78a83265183550c66a78ff9b5f9b960f", "name": "Mila", "category": "unknown"}, {"institution_id": "7c73e6c07b58e2198ed49ebb6e6e6bd0", "name": "Mila, Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "0d655f6117b6a827c084765ed1785481", "name": "ICLR.cc/2024/Conference", "type": "conference", "date": {"text": "2024-01-16", "timestamp": 1705381200, "precision": 3}, "links": [{"type": "openreview-venue", "link": "ICLR.cc/2024/Conference"}], "publisher": null, "series": "", "volume": "ICLR 2024"}, "peer_reviewed": true, "status": "poster", "pages": null}, {"venue": {"venue_id": "6b00fec8c0d7598bda0e96167eaabbc1", "name": "ICLR", "type": "journal", "date": {"text": "2024", "timestamp": 1704085200, "precision": 1}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Computer Science"}, {"name": "Diffusion Models"}, {"name": "Reinforcement Learning"}], "links": [{"type": "doi.abstract", "link": "10.48550/arXiv.2309.06599", "url": "https://doi.org/10.48550/arXiv.2309.06599"}, {"type": "openreview.abstract", "link": "tGQirjzddO", "url": "https://openreview.net/forum?id=tGQirjzddO"}, {"type": "openreview.pdf", "link": "tGQirjzddO", "url": "https://openreview.net/pdf?id=tGQirjzddO"}, {"type": "arxiv.abstract", "link": "2309.06599", "url": "https://arxiv.org/abs/2309.06599"}, {"type": "arxiv.pdf", "link": "2309.06599", "url": "https://arxiv.org/pdf/2309.06599.pdf"}, {"type": "dblp.abstract", "link": "conf/iclr/VenkatramanKAD024", "url": "https://dblp.uni-trier.de/rec/conf/iclr/VenkatramanKAD024"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2309-06599", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2309-06599"}, {"type": "pdf", "link": "https://export.arxiv.org/pdf/2309.06599"}, {"type": "html", "link": "https://openreview.net/forum?id=tGQirjzddO"}, {"type": "semantic_scholar.abstract", "link": "bcc5820c7a84f84347bbf1062dcf7330fe2b0870", "url": "https://www.semanticscholar.org/paper/bcc5820c7a84f84347bbf1062dcf7330fe2b0870"}, {"type": "corpusid", "link": "261706069"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "c4da78496c58d83bf9830a95c3dea273", "title": "Robust and Versatile Bipedal Jumping Control through Reinforcement Learning", "abstract": "", "authors": [{"author": {"author_id": "93cc03207aee2f4d30a097baf4d0cc7e", "name": "Zhongyu Li", "links": [{"type": "semantic_scholar", "link": "1491078398"}, {"type": "xplore", "link": "37088691308"}]}, "affiliations": [{"institution_id": "740d738029438f70cc5112151169933d", "name": "University", "category": "academia"}]}, {"author": {"author_id": "c29bd8289e233e797e229436c937a3eb", "name": "Xue Bin Peng", "links": [{"type": "semantic_scholar", "link": "32200465"}, {"type": "xplore", "link": "37086454470"}]}, "affiliations": []}, {"author": {"author_id": "b69280fba51397fdc283d1a049b765db", "name": "Pieter Abbeel", "links": [{"type": "openreview", "link": "~Pieter_Abbeel2"}, {"type": "semantic_scholar", "link": "1689992"}, {"type": "semantic_scholar", "link": "2262214983"}, {"type": "semantic_scholar", "link": "2279021699"}, {"type": "xplore", "link": "37542877900"}]}, "affiliations": [{"institution_id": "740d738029438f70cc5112151169933d", "name": "University", "category": "academia"}]}, {"author": {"author_id": "db8d2cd3248301ddbe53e438c1f820a5", "name": "Sergey Levine", "links": [{"type": "openreview", "link": "~Sergey_Levine1"}, {"type": "semantic_scholar", "link": "1736651"}, {"type": "semantic_scholar", "link": "2249615151"}, {"type": "semantic_scholar", "link": "2257194331"}, {"type": "semantic_scholar", "link": "2279022150"}, {"type": "xplore", "link": "37085481973"}]}, "affiliations": [{"institution_id": "740d738029438f70cc5112151169933d", "name": "University", "category": "academia"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "200bed81461cad607d593360fe5fcc0a", "name": "Universit\u00e9 de Montr\u00e9al", "category": "unknown"}]}, {"author": {"author_id": "823991839546214e31e46c9c086203cb", "name": "Koushil Sreenath", "links": [{"type": "orcid", "link": "0000-0002-5346-3637"}, {"type": "semantic_scholar", "link": "144116765"}, {"type": "xplore", "link": "37563179200"}]}, "affiliations": [{"institution_id": "740d738029438f70cc5112151169933d", "name": "University", "category": "academia"}]}], "releases": [{"venue": {"venue_id": "143d75cf9bc255f3a82028eba145ceab", "name": "Robotics: Science and Systems XIX", "type": "conference", "date": {"text": "2023-07-10", "timestamp": 1688961600, "precision": 3}, "links": [], "publisher": null, "series": "", "volume": null}, "peer_reviewed": true, "status": "published", "pages": null}], "topics": [{"name": "Computer Science"}, {"name": "Engineering"}], "links": [{"type": "doi.abstract", "link": "10.15607/RSS.2023.XIX.052", "url": "https://doi.org/10.15607/RSS.2023.XIX.052"}, {"type": "arxiv.abstract", "link": "2302.09450", "url": "https://arxiv.org/abs/2302.09450"}, {"type": "arxiv.pdf", "link": "2302.09450", "url": "https://arxiv.org/pdf/2302.09450.pdf"}, {"type": "dblp.abstract", "link": "conf/rss/LiPALBS23", "url": "https://dblp.uni-trier.de/rec/conf/rss/LiPALBS23"}, {"type": "pdf", "link": "https://doi.org/10.15607/rss.2023.xix.052"}, {"type": "semantic_scholar.abstract", "link": "9fda8dbbc030dbf9dae798b051505756be6ffd3a", "url": "https://www.semanticscholar.org/paper/9fda8dbbc030dbf9dae798b051505756be6ffd3a"}, {"type": "corpusid", "link": "259000031"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "8930a484d8aad1cf022ec0f1a62713d5", "title": "Maximum State Entropy Exploration using Predecessor and Successor Representations", "abstract": "Animals have a developed ability to explore that aids them in important tasks such as locating food, exploring for shelter, and finding misplaced items. These exploration skills necessarily track where they have been so that they can plan for finding items with relative efficiency. Contemporary exploration algorithms often learn a less efficient exploration strategy because they either condition only on the current state or simply rely on making random open-loop exploratory moves. In this work, we propose $\\eta\\psi$-Learning, a method to learn efficient exploratory policies by conditioning on past episodic experience to make the next exploratory move. Specifically, $\\eta\\psi$-Learning learns an exploration policy that maximizes the entropy of the state visitation distribution of a single trajectory. Furthermore, we demonstrate how variants of the predecessor representation and successor representations can be combined to predict the state visitation entropy. Our experiments demonstrate the efficacy of $\\eta\\psi$-Learning to strategically explore the environment and maximize the state coverage with limited samples.", "authors": [{"author": {"author_id": "a873db153082ee433ec02f71f0aaa702", "name": "Arnav Kumar Jain", "links": [{"type": "openreview", "link": "~Arnav_Kumar_Jain2"}, {"type": "semantic_scholar", "link": "7284555"}]}, "affiliations": [{"institution_id": "099386ee291a46438cd2f3c2e05267c8", "name": "University de Montreal", "category": "academia"}]}, {"author": {"author_id": "ce65563792a1927b992b7801c86797e8", "name": "Lucas Lehnert", "links": [{"type": "openreview", "link": "~Lucas_Lehnert1"}, {"type": "semantic_scholar", "link": "2284988047"}, {"type": "semantic_scholar", "link": "39251318"}]}, "affiliations": [{"institution_id": "53ebc9c43621e396de19895e697cb413", "name": "Meta", "category": "unknown"}]}, {"author": {"author_id": "d7996e27da7f5c147d15bf0ee70b7c0c", "name": "Irina Rish", "links": [{"type": "!semantic_scholar", "link": "2064747582"}, {"type": "bio", "link": "irina-rish"}, {"type": "email.mila", "link": "irina.rish@mila.quebec"}, {"type": "mag", "link": "1653753694"}, {"type": "mag", "link": "2997858046"}, {"type": "mag", "link": "3025417811"}, {"type": "openreview", "link": "~Irina_Rish1"}, {"type": "semantic_scholar", "link": "2109771"}, {"type": "semantic_scholar", "link": "2239232896"}, {"type": "semantic_scholar", "link": "2284772297"}, {"type": "wpid_en", "link": "37584"}, {"type": "wpid_fr", "link": "37580"}, {"type": "xplore", "link": "37268997000"}]}, "affiliations": [{"institution_id": "099386ee291a46438cd2f3c2e05267c8", "name": "University de Montreal", "category": "academia"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "099386ee291a46438cd2f3c2e05267c8", "name": "University de Montreal", "category": "academia"}]}], "releases": [{"venue": {"venue_id": "2231305ebc59ffab234d656101ea6714", "name": "NeurIPS.cc/2023/Conference", "type": "conference", "date": {"text": "2023-09-21", "timestamp": 1695268800, "precision": 3}, "links": [{"type": "openreview-venue", "link": "NeurIPS.cc/2023/Conference"}], "publisher": null, "series": "", "volume": "NeurIPS 2023"}, "peer_reviewed": true, "status": "poster", "pages": null}], "topics": [{"name": "Successor Representation"}, {"name": "Computer Science"}, {"name": "State Visitation Distribution"}, {"name": "Maximum state entropy exploration"}, {"name": "Non-Markovian exploration"}, {"name": "Reinforcement Learning"}], "links": [{"type": "html.official", "link": "https://proceedings.neurips.cc//paper_files/paper/2023/hash/9c7900fac04a701cbed83256b76dbaa3-Abstract-Conference.html"}, {"type": "pdf.official", "link": "https://proceedings.neurips.cc/paper_files/paper/2023/file/9c7900fac04a701cbed83256b76dbaa3-Paper-Conference.pdf"}, {"type": "doi.abstract", "link": "10.48550/arXiv.2306.14808", "url": "https://doi.org/10.48550/arXiv.2306.14808"}, {"type": "openreview.abstract", "link": "inE5hW4tQ0", "url": "https://openreview.net/forum?id=inE5hW4tQ0"}, {"type": "openreview.abstract", "link": "tFsxtqGmkn", "url": "https://openreview.net/forum?id=tFsxtqGmkn"}, {"type": "openreview.pdf", "link": "inE5hW4tQ0", "url": "https://openreview.net/pdf?id=inE5hW4tQ0"}, {"type": "openreview.pdf", "link": "tFsxtqGmkn", "url": "https://openreview.net/pdf?id=tFsxtqGmkn"}, {"type": "arxiv.abstract", "link": "2306.14808", "url": "https://arxiv.org/abs/2306.14808"}, {"type": "arxiv.pdf", "link": "2306.14808", "url": "https://arxiv.org/pdf/2306.14808.pdf"}, {"type": "dblp.abstract", "link": "journals/corr/abs-2306-14808", "url": "https://dblp.uni-trier.de/rec/journals/corr/abs-2306-14808"}, {"type": "pdf", "link": "http://export.arxiv.org/pdf/2306.14808"}, {"type": "semantic_scholar.abstract", "link": "6b8e98792e4af57687939156c07b99cd12187f89", "url": "https://www.semanticscholar.org/paper/6b8e98792e4af57687939156c07b99cd12187f89"}, {"type": "corpusid", "link": "259261970"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}, {"paper_id": "81709b4783324a59fd2632ee694e9071", "title": "Heterogeneous Crowd Simulation Using Parametric Reinforcement Learning", "abstract": "Agent-based synthetic crowd simulation affords the cost-effective large-scale simulation and animation of interacting digital humans. Model-based approaches have successfully generated a plethora of simulators with a variety of foundations. However, prior approaches have been based on statically defined models predicated on simplifying assumptions, limited video-based datasets, or homogeneous policies. Recent works have applied reinforcement learning to learn policies for navigation. However, these approaches may learn static homogeneous rules, are typically limited in their generalization to trained scenarios, and limited in their usability in synthetic crowd domains. In this article, we present a multi-agent reinforcement learning-based approach that learns a parametric predictive collision avoidance and steering policy. We show that training over a parameter space produces a flexible model across crowd configurations. That is, our goal-conditioned approach learns a parametric policy that affords heterogeneous synthetic crowds. We propose a model-free approach without centralization of internal agent information, control signals, or agent communication. The model is extensively evaluated. The results show policy generalization across unseen scenarios, agent parameters, and out-of-distribution parameterizations. The learned model has comparable computational performance to traditional methods. Qualitatively the model produces both expected (laminar flow, shuffling, bottleneck) and unexpected (side-stepping) emergent qualitative behaviours, and quantitatively the approach is performant across measures of movement quality.", "authors": [{"author": {"author_id": "c0c032d3b7ff5af19be908b1751a23cf", "name": "Kaidong Hu", "links": [{"type": "semantic_scholar", "link": "50726531"}, {"type": "xplore", "link": "37089746546"}]}, "affiliations": [{"institution_id": "28cadb31618609307cd35ba86b62139e", "name": "Computer Science Department, Rutgers University, New Brunswick, NJ, USA", "category": "unknown"}]}, {"author": {"author_id": "bc42a5fdcccf1367b1c656a5fcb18881", "name": "Brandon Haworth", "links": [{"type": "semantic_scholar", "link": "1398795708"}, {"type": "xplore", "link": "37085866314"}]}, "affiliations": [{"institution_id": "27b84398ca0392c69183ebb2995392d8", "name": "Department of Computer Science, University of Victoria, Victoria, BC, Canada", "category": "unknown"}]}, {"author": {"author_id": "adadd4e014e1512d15decd94c364e653", "name": "Glen Berseth", "links": [{"type": "bio", "link": "glen-berseth"}, {"type": "email.mila", "link": "glen.berseth@mila.quebec"}, {"type": "mag", "link": "887882191"}, {"type": "openreview", "link": "~Glen_Berseth1"}, {"type": "semantic_scholar", "link": "2253652688"}, {"type": "semantic_scholar", "link": "2262214826"}, {"type": "semantic_scholar", "link": "2304321940"}, {"type": "semantic_scholar", "link": "2305596832"}, {"type": "semantic_scholar", "link": "2994035"}, {"type": "wpid_en", "link": "46450"}, {"type": "wpid_fr", "link": "46455"}, {"type": "xplore", "link": "37085864638"}]}, "affiliations": [{"institution_id": "54cfd71773d34529369bc6b5d1feae82", "name": "Department of Computer Science and Operations Research, University of Montreal and a member of MILA, Montreal, Quebec, Canada", "category": "unknown"}]}, {"author": {"author_id": "d6c8126cfcf71d7b17934db72b278628", "name": "Vladimir Pavlovic", "links": [{"type": "semantic_scholar", "link": "144658464"}, {"type": "xplore", "link": "37270545400"}]}, "affiliations": [{"institution_id": "28cadb31618609307cd35ba86b62139e", "name": "Computer Science Department, Rutgers University, New Brunswick, NJ, USA", "category": "unknown"}]}, {"author": {"author_id": "d90b1bc67a3bec17efb0c978fae79d8c", "name": "Petros Faloutsos", "links": [{"type": "semantic_scholar", "link": "1737527"}, {"type": "xplore", "link": "37448800400"}]}, "affiliations": [{"institution_id": "3b48df60a957a8b1ea61583627ce412b", "name": "Department of Electrical Engineering & Computer Science, York University, Toronto, ON, Canada", "category": "unknown"}]}, {"author": {"author_id": "c97ec0b5b6cfbf7b10a9400aea00ec93", "name": "Mubbasir Kapadia", "links": [{"type": "semantic_scholar", "link": "143980996"}, {"type": "semantic_scholar", "link": "143980997"}, {"type": "xplore", "link": "37714469100"}]}, "affiliations": [{"institution_id": "28cadb31618609307cd35ba86b62139e", "name": "Computer Science Department, Rutgers University, New Brunswick, NJ, USA", "category": "unknown"}]}], "releases": [{"venue": {"venue_id": "088d762262ce0b6c6420e40642f96eb5", "name": "IEEE Transactions on Visualization and Computer Graphics", "type": "journal", "date": {"text": "2023-04-01", "timestamp": 1680321600, "precision": 3}, "links": [], "publisher": "IEEE", "series": "", "volume": "29"}, "peer_reviewed": true, "status": "published", "pages": "2036-2052"}], "topics": [{"name": "Learnable Parameters"}, {"name": "Medicine"}, {"name": "Emergent Behavior"}, {"name": "Multi-agent Learning"}, {"name": "Multiple Agents"}, {"name": "Long Short Term Memory"}, {"name": "Computer Science"}, {"name": "parametric policy learning"}, {"name": "Random Initialization"}, {"name": "Navigation"}, {"name": "Predictive models"}, {"name": "Neural networks"}, {"name": "Multi-agent navigation"}, {"name": "Heterogeneity Scenarios"}, {"name": "Number Of Agents"}, {"name": "Completion Time"}, {"name": "Markov Decision Process"}, {"name": "Deep Learning"}, {"name": "reinforcement learning"}, {"name": "Policy Learning"}, {"name": "Model-free Approach"}, {"name": "State Space"}, {"name": "Current Frame"}, {"name": "Neural Network"}, {"name": "Heterogeneity Of Agents"}, {"name": "State Observer"}, {"name": "Environment Configuration"}, {"name": "Part Of The State"}, {"name": "Heterogeneous Scenarios"}, {"name": "Training"}, {"name": "Computational Performance"}, {"name": "State-space"}, {"name": "Development Literature"}, {"name": "crowd simulation"}, {"name": "Training Environment"}, {"name": "Crowd Simulation"}, {"name": "Reinforcement learning"}, {"name": "Computational modeling"}, {"name": "Collision avoidance"}, {"name": "Historical Literature"}, {"name": "Long Short-term Memory"}, {"name": "Heterogeneous Agents"}, {"name": "Deep Reinforcement Learning"}, {"name": "Single Policy"}, {"name": "Reward Function"}, {"name": "Multi-agent Reinforcement Learning"}, {"name": "Private Space"}, {"name": "LSTM"}, {"name": "Personal Space"}, {"name": "Emergent Behaviour"}, {"name": "Source Model"}, {"name": "Proximal Policy Optimization"}], "links": [{"type": "doi.abstract", "link": "10.1109/TVCG.2021.3139031", "url": "https://doi.org/10.1109/TVCG.2021.3139031"}, {"type": "pubmed.abstract", "link": "34965213", "url": "https://pubmed.ncbi.nlm.nih.gov/34965213"}, {"type": "dblp.abstract", "link": "journals/tvcg/HuHBPFK23", "url": "https://dblp.uni-trier.de/rec/journals/tvcg/HuHBPFK23"}, {"type": "pdf", "link": "https://dspace.library.uvic.ca/bitstream/1828/15134/1/Haworth_IEEE_Trans._Vis._Comput._2021.pdf"}, {"type": "semantic_scholar.abstract", "link": "cadba12b17a89512e9553801b0ed2f640f832f67", "url": "https://www.semanticscholar.org/paper/cadba12b17a89512e9553801b0ed2f640f832f67"}, {"type": "corpusid", "link": "245566720"}], "flags": [{"name": "validation", "value": 1}], "validated": true, "citation_count": 0, "excerpt": null}]
